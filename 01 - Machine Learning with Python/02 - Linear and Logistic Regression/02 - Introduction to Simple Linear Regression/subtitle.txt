Welcome to this video on Introduction to Simple Linear Regression. After watching this video, you will be able to describe simple linear regression and explain how simple linear regression works. Linear regression models a linear relationship between a continuous target variable and explanatory features. Consider this dataset related to CO2 emissions from different cars. The dataset features engine size, number of cylinders, fuel consumption, and CO2 emissions from various cars. Given this dataset, linear regression can be used to predict a continuous value, such as CO2 emissions of a car. In simple linear regression, a single independent variable estimates the dependent variable. For example, in our dataset, CO2 emission is predicted using the engine size variable. Let's consider the same dataset again. Let's plot engine size as an independent variable and CO2 emissions as the target value that we want to predict. A scatter plot clearly shows the correlation between variables where changes in one variable explain or possibly cause changes in the other variable. With simple linear regression, you can determine a best-fit line through the data. As engine size increases, so does CO2 emissions, and the relationship is approximately linear. You can use simple linear regression to predict the emission of an unknown car. For example, the predicted emission for a sample car with an engine size of 2.4 is 214. You can predict the target value, CO2 emissions, represented as the response variable, y-hat. The independent variable, which in this case is engine size, is represented by the single predictor variable, x1. The model is represented as the equation of a line here. y-hat is the predicted response expressed in terms of x1 using a y-intercept, theta zero, and a slope, theta one. Theta zero and theta one are called the coefficients of the linear regression model, selected by the linear regression algorithm to determine a best-fit line. Let's consider some new data points and check how well they align with the regression line. Given a car with engine size x1 equals 5.4, its actual CO2 emission is 50, while its predicted emission is y-hat equals 340. Comparing the actual value to the predicted one, there's a 90-unit discrepancy. The residual error is the vertical distance from the data point to the fitted regression line. The average of all residual errors measures how poorly the regression line fits the data. Mathematically, it can be shown by the equation mean squared error, shown as MSE. Linear regression aims to find the line for minimizing the mean of all these residual errors. This form of regression is commonly known as ordinary least squares regression, or OLS regression. We can use two formulas to calculate the coefficients theta zero and theta one of the linear regression model. The solution was derived independently by the mathematicians Gauss and Legendre in the early 1800s. It requires that we calculate the means, y-bar and x-bar, of the independent and dependent variables. The xi and yi in the equation for theta one refer to the ith values of x and y. Here, you can calculate the x-bar as 3.0 and the y-bar as 226.2. Then, going through the calculations, you'll find that theta one equals 39. You can use this result to calculate the first parameter line intercept as 125.7. So, these are the two parameters for the line, where theta zero is also called the bias coefficient and theta one is the coefficient for the CO2 emission column. Given the parameters of the linear equation, making a prediction is as simple as solving the equation for a particular input value. For example, you can predict the CO2 emission from engine size for the automobile in record number 9 using the following equation. For an engine size of 2.4, we can predict that the CO2 emission of the car would be 214. The OLS regression method is helpful because it's easy to understand and interpret. The method doesn't require any tuning and its solution is just a calculation. This also makes OLS regression fast, especially for smaller datasets. On the other hand, a linear model may be far too simplistic to capture complexity, such as a nonlinear relationship in the data. Outliers can greatly reduce its accuracy, giving them far too much weight in the calculations. In this video, we looked at several use cases of simple linear regression. We learned how to predict a continuous value, such as a car's CO2 emissions. In simple linear regression, a single independent variable estimates the dependent variable. We also learned how to determine the best fit line through a chart showing regression values. We learned about the concept of Mean Squared Error, or MSE, which measures how poorly the regression line fits the data. Linear regression aims to find the line for minimizing the mean of all these residual errors. This form of regression is commonly known as Ordinary Least Squares Regression, or OLS regression. The OLS regression method is useful because it's easy to understand and interpret. However, outliers can greatly reduce its accuracy, giving them far too much weight in the calculations.