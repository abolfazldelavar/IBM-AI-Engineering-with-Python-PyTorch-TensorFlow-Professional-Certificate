Welcome to Introduction to Logistic Regression. After watching this video, you will be able to describe the machine learning method of logistic regression and explain how it is used. Logistic regression is a statistical modeling technique that predicts the probability of an observation belonging to one or two classes, such as true or false. In machine learning, logistic regression refers to a binary classifier based on statistical logistic regression. By choosing a threshold probability, the probability predictor becomes a binary classifier simply by assigning each observation to one class if its probability is greater than the threshold and to the other class if its probability is less than the threshold. Let's understand when logistic regression is a good choice. First, when the target in your data is binary, indicated as 0 or 1. Second, when you need the probability of an outcome, like the probability of a customer buying a product. If the data is linearly separable, the decision boundary of logistic regression is a line, a plane, or a hyperplane. Example, theta 0 plus theta 1 x 1 plus theta 2 x 2 is greater than 0. Third, when you want to understand the impact of an independent feature, it allows you to select the best features based on the size of their model coefficients or weights. Logistic regression is both a probability predictor and a binary classifier. For example, you can use it to predict the probability that a person will have a heart attack within a specified time period based on knowledge of the person's age, sex, and body mass index, chance that a specific condition or disease, such as diabetes, might appear based on observed characteristics of that patient, such as weight, height, blood pressure, and results of various blood tests, and so on, likelihood that a customer of a subscription-based service will halt its subscription, probability of failure of a given process, system, or product, and likelihood of a homeowner defaulting on a mortgage. Consider a telecommunication dataset that you'd like to analyze to predict which customers might leave next month. The dataset comprises historical customer data, where each row represents one customer. The data includes information about services that each customer has signed up for, customer account information, demographic information like gender and age range, and customers who've churned or left within the last month. Here, the dependent variable column is called churn. In logistics regression, you can use one or more of these features to predict whether customers will churn. Logistic regression can predict the class, y-hat, of each customer by considering the predicted probability, p-hat, that the customer will churn. Here, p-hat is the predicted probability that the class, y is 1, given the data, x. Suppose the goal is to predict customer churn based on their age. You have a feature, age, denoted as x, and a binary target, variable churn, denoted as y, with two classes, yes and no, represented by binary values 1 and 0. Graphically, you can represent the data with a scatterplot, where class 0 is denoted in red and class 1 in blue. With linear regression, you can fit a line through the data represented as y-hat equals theta 0 plus theta 1 x 1. This line has two parameters, where theta 0 is the y-intercept of the line, and theta 1 is its slope. Theta 1 is also called the weight vector, or confidence of the equation. One problem is that the prediction's value y-hat increases indefinitely with age, because the line goes on forever. Obviously, you cannot use linear regression directly to predict churn. Somehow, the predicted values need to be contained within the range 0 to 1. One way to keep the predicted values within the range of 0 to 1 is to use a threshold, like 0.5, to differentiate the classes. You can write a rule to allow you to separate class 0 from class 1. If the value of y-hat is less than 0.5, then the class is 0, otherwise the class is 1. Notice that in the step function, no matter how big the value is, as long as it's greater than 0.5, it simply equals 1. And regardless of how negative the value y-hat is, the output is 0 if it is less than 0.5. In other words, there is no difference between a customer 20 years old and 100 years old. The outcome would be 1. Wouldn't it be nice to use a smoother line that would project these values between 0 and 1? Indeed, the existing method doesn't provide the probability of a customer belonging to a class, which is the goal. Consider the sigmoid function, sigma of x, also known as the logit function, defined as 1 over the sum of 1 and e to the minus x. This graph shows that for x equals 0, the sigmoid function is 0.5. As x grows, the sigmoid function approaches 1, and as x becomes more negative, the sigmoid approaches 0. This means that the sigmoid function can take any continuous function of x and continuously compress it within the range 0, 1. It defines a probability. Now the model is sigma of y hat, which represents the probability p hat, and that the output is 1 given x. Thus you can determine the chance that an observation belongs to either class. To assign the class to the observation, simply define a threshold like 0.5 and assign the observation to 1 if its probability is greater than 0.5 and 0 otherwise. This threshold is known as a decision boundary. What is the output of the customer churn model when we use the sigmoid function? The churn probability is denoted as the probability that y is 1 given the data x. Notice that the probability that the customer won't churn is 1 minus the churn probability, as the two probabilities must add to 1. For example, suppose the probability of a customer staying with the company can be shown as the probability of churn given a customer's income and age, which could be 0.8. Then, the probability of the same customer staying is 1 minus 0.8, which is 0.2. In this video, you learned that in machine learning, logistic regression refers to a binary classifier based on statistical logistic regression, or probability predictor. Logistic regression is a good choice for a binary target, probabilistic results, and understanding the impact of a feature. You also learned that logistics regression is both a probability predictor and a binary classifier. The goal of logistic regression is to build a model to predict the class by considering the predicted probability.