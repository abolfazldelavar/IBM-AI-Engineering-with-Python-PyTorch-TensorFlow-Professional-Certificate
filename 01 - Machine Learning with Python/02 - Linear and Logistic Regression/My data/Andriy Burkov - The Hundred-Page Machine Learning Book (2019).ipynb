{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734a0350",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Mathematical Summary â€” *The Hundred-Page Machine Learning Book* (Andriy Burkov)\n",
    "\n",
    "> **Purpose:** a compact, equation-centered cheat-sheet covering the major ML concepts presented in the book.\n",
    "---\n",
    "\n",
    "## 1. Problem Setup & Notation\n",
    "\n",
    "- Dataset: $D = \\{(x_i, y_i)\\}_{i=1}^n$, where $x_i \\in \\mathbb{R}^d$, $y_i \\in \\mathcal{Y}$.  \n",
    "- Supervised learning: learn $f:\\mathbb{R}^d \\to \\mathcal{Y}$ to minimize expected loss $\\mathbb{E}[\\ell(y, f(x))]$.  \n",
    "- Empirical risk (training loss):\n",
    "  $$\n",
    "  \\hat{R}(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\ell(y_i, f(x_i;\\theta))\n",
    "  $$\n",
    "- Goal: $\\theta^* = \\arg\\min_\\theta \\hat{R}(\\theta)$ (approximate population minimizer).\n",
    "\n",
    "**Tip:** replace $\\hat{R}$ with specific loss (MSE, cross-entropy) to derive algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Linear Regression (OLS)\n",
    "\n",
    "- Model: $y = X\\beta + \\varepsilon$, $X \\in \\mathbb{R}^{n\\times (d+1)}$ (column of ones for intercept).\n",
    "- Mean Squared Error (MSE) objective:\n",
    "  $$\n",
    "  \\hat{R}(\\beta) = \\frac{1}{n}\\|y - X\\beta\\|_2^2\n",
    "  $$\n",
    "- Closed-form solution (Ordinary Least Squares):\n",
    "  $$\n",
    "  \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y \\quad\\text{(if $X^\\top X$ invertible)}\n",
    "  $$\n",
    "- Gradient (for iterative solvers):\n",
    "  $$\n",
    "  \\nabla_\\beta \\hat{R} = -\\frac{2}{n} X^\\top (y - X\\beta)\n",
    "  $$\n",
    "- **Ridge (L2) regularization**:\n",
    "  $$\n",
    "  \\hat{\\beta}_{\\text{ridge}} = \\arg\\min_\\beta \\frac{1}{n}\\|y-X\\beta\\|_2^2 + \\lambda\\|\\beta\\|_2^2\n",
    "  $$\n",
    "  closed-form:\n",
    "  $$\n",
    "  \\hat{\\beta}_{\\text{ridge}} = (X^\\top X + n\\lambda I)^{-1} X^\\top y\n",
    "  $$\n",
    "- **Lasso (L1)**:\n",
    "  $$\n",
    "  \\arg\\min_\\beta \\frac{1}{n}\\|y-X\\beta\\|_2^2 + \\alpha \\|\\beta\\|_1\n",
    "  $$\n",
    "  -> no closed form; use coordinate descent.\n",
    "\n",
    "**Note:** L2 shrinks coefficients; L1 encourages sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Logistic Regression (Binary)\n",
    "\n",
    "- Model: $P(y=1|x)=\\sigma(w^\\top x)$ with $\\sigma(z) = \\frac{1}{1+e^{-z}}$.\n",
    "- Log-loss / cross-entropy:\n",
    "  $$\n",
    "  \\ell(y, \\hat{p}) = -y\\log \\hat{p} - (1-y)\\log(1-\\hat{p})\n",
    "  $$\n",
    "- Objective (neg log-likelihood):\n",
    "  $$\n",
    "  L(w) = -\\frac{1}{n}\\sum_{i=1}^n \\left[y_i \\log \\sigma(w^\\top x_i) + (1-y_i)\\log(1-\\sigma(w^\\top x_i))\\right]\n",
    "  $$\n",
    "- Gradient:\n",
    "  $$\n",
    "  \\nabla_w L = -\\frac{1}{n} \\sum_{i=1}^n (y_i - \\sigma(w^\\top x_i)) x_i\n",
    "  $$\n",
    "- Regularize with L2 or L1 similarly.\n",
    "\n",
    "**Tip:** Logistic regression = linear model + cross-entropy; optimization via gradient-based methods.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Losses, Likelihood & Bayesian View\n",
    "\n",
    "- MSE arises from Gaussian noise assumption: $y|x \\sim \\mathcal{N}(w^\\top x, \\sigma^2)$. MLE $\\to$ minimize squared error.\n",
    "- Cross-entropy corresponds to Bernoulli/Categorical likelihood.\n",
    "- Bayesian estimation: posterior\n",
    "  $$\n",
    "  p(\\theta|D) \\propto p(D|\\theta)p(\\theta)\n",
    "  $$\n",
    "  MAP estimate: $\\theta_{\\text{MAP}} = \\arg\\max_\\theta \\log p(D|\\theta) + \\log p(\\theta)$ (regularized MLE).\n",
    "\n",
    "**Note:** Prior acts as regularizer (Gaussian prior $\\to$ L2 ridge, Laplace prior $\\to$ L1 lasso).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Optimization Algorithms\n",
    "\n",
    "- **Gradient Descent (GD)**:\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\hat{R}(\\theta_t)\n",
    "  $$\n",
    "- **Stochastic Gradient Descent (SGD)**:\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\ell(y_i, f(x_i; \\theta_t))\n",
    "  $$\n",
    "- **Mini-batch**, momentum, Nesterov, Adam:\n",
    "  - Momentum: $v_{t+1} = \\mu v_t - \\eta \\nabla$, $\\theta_{t+1} = \\theta_t + v_{t+1}$.\n",
    "  - Adam keeps adaptive estimates of first/second moments.\n",
    "\n",
    "**Tip:** learning rate $\\eta$ schedule matters more than algorithm choice in many cases.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Biasâ€“Variance Decomposition (Squared Loss)\n",
    "\n",
    "For estimator $\\hat{f}(x)$,\n",
    "$$\n",
    "\\mathbb{E}[(y - \\hat{f}(x))^2] = \\underbrace{\\left(\\mathbb{E}[\\hat{f}(x)] - f^*(x)\\right)^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]}_{\\text{Variance}} + \\text{Noise}\n",
    "$$\n",
    "\n",
    "- Trade-off: more complex models reduce bias but increase variance.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Model Selection & Evaluation\n",
    "\n",
    "- **Train/Validation/Test** split. Cross-validation ($k$-fold) estimate generalization.\n",
    "- Metrics:\n",
    "  - Regression: MSE, RMSE, MAE, $R^2$.\n",
    "  - Classification: accuracy; confusion matrix\n",
    "    $$\n",
    "    \\text{precision} = \\frac{TP}{TP+FP},\\quad \\text{recall}=\\frac{TP}{TP+FN}\n",
    "    $$\n",
    "    $$\n",
    "    F_1 = 2\\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision}+\\text{recall}}\n",
    "    $$\n",
    "  - ROC AUC for probabilistic classifiers.\n",
    "\n",
    "**Tip:** use proper metric for imbalanced data (precision/recall, ROC AUC).\n",
    "\n",
    "---\n",
    "\n",
    "## 8. k-Nearest Neighbors (kNN)\n",
    "\n",
    "- Predict by averaging labels of $k$ nearest neighbors (distance typically Euclidean).  \n",
    "- No training; inference complexity large for big datasets.\n",
    "- **Curse of dimensionality:** distances lose meaning with high $d$.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Support Vector Machines (SVM)\n",
    "\n",
    "- Binary linear SVM primal (soft margin):\n",
    "  $$\n",
    "  \\min_{w,b,\\xi} \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^n \\xi_i\n",
    "  $$\n",
    "  subject to $y_i (w^\\top x_i + b) \\ge 1 - \\xi_i$, $\\xi_i\\ge0$.\n",
    "- Hinge loss formulation:\n",
    "  $$\n",
    "  \\min_w \\frac{1}{2}\\|w\\|^2 + C\\sum_{i}\\max(0, 1 - y_i w^\\top x_i)\n",
    "  $$\n",
    "- Kernel trick: replace inner product by kernel $K(x, x')$ to operate in feature space.\n",
    "\n",
    "**Note:** SVM maximizes margin; $C$ trades margin vs. slack (regularization).\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Decision Trees & Ensembles\n",
    "\n",
    "- **Decision tree**: recursively partition feature space; impurity measures (Gini, entropy).\n",
    "- **Random Forest**: ensemble of trees grown on bootstrap samples with feature subsampling; predictions averaged (regression) or majority vote (classification).\n",
    "- **Boosting (e.g., AdaBoost, Gradient Boosting)**: sequentially fit learners to residuals or reweighted data:\n",
    "  - Gradient boosting objective: add model $h_m$ minimizing gradient of loss:\n",
    "    $$\n",
    "    F_{m}(x) = F_{m-1}(x) + \\gamma_m h_m(x)\n",
    "    $$\n",
    "  - XGBoost / LightGBM are efficient gradient-boosted tree implementations.\n",
    "\n",
    "**Tip:** forests reduce variance; boosting reduces bias.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Unsupervised Learning â€” PCA & Clustering\n",
    "\n",
    "- **PCA**: find orthogonal directions maximizing variance.\n",
    "  - Covariance $S = \\frac{1}{n} X^\\top X$. Eigen-decomposition: $S v_k = \\lambda_k v_k$.\n",
    "  - Projection: $z = V_k^\\top x$ (retain top-$k$ eigenvectors).\n",
    "  - Equivalent via SVD: $X = U\\Sigma V^\\top$.\n",
    "- **k-Means**: minimize within-cluster sum of squares:\n",
    "  $$\n",
    "  \\min_{C_1,\\dots,C_k} \\sum_{j=1}^k \\sum_{x\\in C_j} \\|x - \\mu_j\\|^2\n",
    "  $$\n",
    "  iterative Lloydâ€™s algorithm: assign/update.\n",
    "- **GMM + EM**:\n",
    "  - E-step: compute responsibilities $\\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(x_i|\\mu_k,\\Sigma_k)}{\\sum_j \\pi_j \\mathcal{N}(x_i|\\mu_j,\\Sigma_j)}$.\n",
    "  - M-step: update $\\pi_k, \\mu_k, \\Sigma_k$ via weighted MLE.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Probabilistic Models â€” Naive Bayes\n",
    "\n",
    "- Bayes rule: $P(y|x) \\propto P(x|y)P(y)$.\n",
    "- **Naive Bayes** assumes conditional independence:\n",
    "  $$\n",
    "  P(x|y) = \\prod_{j=1}^d P(x_j | y)\n",
    "  $$\n",
    "- Classifier: $\\hat{y} = \\arg\\max_y P(y) \\prod_j P(x_j|y)$ (use log-probabilities numerically).\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Expectation-Maximization (EM)\n",
    "\n",
    "- For latent variables $z$,\n",
    "  - E-step: $Q(\\theta|\\theta^{(t)}) = \\mathbb{E}_{z|x,\\theta^{(t)}} [\\log p(x,z|\\theta)]$.\n",
    "  - M-step: $\\theta^{(t+1)} = \\arg\\max_\\theta Q(\\theta|\\theta^{(t)})$.\n",
    "- EM increases marginal likelihood each iteration.\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Neural Networks (Intro)\n",
    "\n",
    "- Feedforward NN: layers with activations. For one hidden layer:\n",
    "  $$\n",
    "  \\hat{y} = W^{(2)}\\phi(W^{(1)}x + b^{(1)}) + b^{(2)}\n",
    "  $$\n",
    "- Loss: cross-entropy for classification, MSE for regression.\n",
    "- **Backpropagation** uses chain rule to compute gradients efficiently.\n",
    "- Activation examples: ReLU $(x)_+$, sigmoid, tanh, softmax for outputs:\n",
    "  $$\n",
    "  \\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "  $$\n",
    "- **Regularization:** dropout, weight decay (L2), batch normalization.\n",
    "\n",
    "**Tip:** initialization & learning rate scheduling are critical.\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Convolutional & Recurrent Nets (Short)\n",
    "\n",
    "- **CNNs:** convolutional layers compute local receptive fields; weight sharing reduces params.\n",
    "- **RNNs / LSTM / GRU:** handle sequences; gradients can vanish/explode; gating mitigates issues.\n",
    "\n",
    "---\n",
    "\n",
    "## 16. Representation Learning & Embeddings\n",
    "\n",
    "- Word/image embeddings are learned vector representations.  \n",
    "- Word2Vec objective (skip-gram, negative sampling) approximates PMI-style relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## 17. Reinforcement Learning (Very Brief)\n",
    "\n",
    "- Agent interacts with environment; seeks policy $\\pi(a|s)$ to maximize expected return $\\mathbb{E}[\\sum_t \\gamma^t r_t]$.\n",
    "- Key equations: Bellman expectation for value $V^\\pi(s)$:\n",
    "  $$\n",
    "  V^\\pi(s) = \\mathbb{E}_{a\\sim\\pi, s'\\sim P} [r(s,a) + \\gamma V^\\pi(s')]\n",
    "  $$\n",
    "- Policy iteration, value iteration, Q-learning: $Q(s,a) \\leftarrow Q + \\alpha (r + \\gamma \\max_a Q - Q)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 18. Practical Data Prep & Feature Engineering\n",
    "\n",
    "- **Scaling:** standardize features: $x' = \\frac{x-\\mu}{\\sigma}$. Many algorithms (SVM, kNN, neural nets) need scaling.\n",
    "- **Categorical:** one-hot encoding or learned embeddings.\n",
    "- **Missing values:** imputation strategies (mean, median, model-based).\n",
    "- **Feature selection:** filter, wrapper, embedded methods.\n",
    "- **Pipeline:** chain preprocessing + model to avoid leakage.\n",
    "\n",
    "**Tip:** always fit preprocessing on training set only.\n",
    "\n",
    "---\n",
    "\n",
    "## 19. Calibration & Probabilities\n",
    "\n",
    "- Some classifiers (SVM, tree ensembles) need probability calibration (Platt scaling, isotonic regression).\n",
    "- Proper scoring rule: log-loss encourages good probability estimates.\n",
    "\n",
    "---\n",
    "\n",
    "## 20. Model Interpretability & Uncertainty\n",
    "\n",
    "- Linear models: coefficients have direct interpretation (with caveats).\n",
    "- Tree-based models: feature importance, partial dependence plots.\n",
    "- Uncertainty: Bayesian methods, ensembles, or MC dropout approximate predictive uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "## 21. Quick Reference â€” Key Equations\n",
    "\n",
    "- OLS:\n",
    "  $$\n",
    "  \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n",
    "  $$\n",
    "- Ridge:\n",
    "  $$\n",
    "  \\hat{\\beta} = (X^\\top X + n\\lambda I)^{-1}X^\\top y\n",
    "  $$\n",
    "- Logistic loss gradient:\n",
    "  $$\n",
    "  \\nabla_w = -\\frac{1}{n}\\sum_i (y_i - \\sigma(w^\\top x_i))x_i\n",
    "  $$\n",
    "- Softmax cross-entropy:\n",
    "  $$\n",
    "  L = -\\frac{1}{n}\\sum_i \\sum_{k} y_{ik}\\log \\frac{e^{z_{ik}}}{\\sum_j e^{z_{ij}}}\n",
    "  $$\n",
    "- PCA via SVD:\n",
    "  $$\n",
    "  X = U\\Sigma V^\\top,\\quad \\text{proj}_k(x)=V_k^\\top x\n",
    "  $$\n",
    "- k-means objective:\n",
    "  $$\n",
    "  \\min_{C,\\mu}\\sum_j\\sum_{x\\in C_j}\\|x-\\mu_j\\|^2\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 22. Practical Tips (Short)\n",
    "\n",
    "- Start with simple baselines (linear/logistic, decision tree, kNN).\n",
    "- Use cross-validation for model selection and hyperparameter tuning.\n",
    "- Regularize to prevent overfitting; prefer simple models if performance similar.\n",
    "- Monitor learning curves (train vs validation error) to diagnose bias/variance.\n",
    "- Scale features for distance-based and gradient-based methods.\n",
    "- For structured/tabular data, gradient-boosted trees often strong baseline.\n",
    "- For large-scale unstructured data (images, audio, text), deep learning is preferred.\n",
    "\n",
    "---\n",
    "\n",
    "## 23. Notes on Computational Complexity\n",
    "\n",
    "- OLS closed form: $O(d^3)$ (matrix inverse) or use iterative $O(nd^2)$ alternatives.  \n",
    "- kNN: training $O(1)$, prediction $O(n d)$ (expensive).  \n",
    "- Trees: training depends on sorting and splits; forest/boosting add multiplicative factors.\n",
    "\n",
    "---\n",
    "\n",
    "## 24. Closing Summary\n",
    "\n",
    "- The book emphasizes pragmatic intuition: match model complexity to data, prefer simple interpretable models, use regularization and validation, and understand underlying assumptions.\n",
    "- Mathematical backbone: optimization (GD/SGD), probabilistic modeling (MLE/MAP), linear algebra (SVD, eigen), and statistics (biasâ€“variance, cross-validation).\n",
    "\n",
    "---\n",
    "\n",
    "## References & Further Reading (legal sources)\n",
    "- Burkov, *The Hundred-Page Machine Learning Book* â€” publisher / official page.  \n",
    "- Goodfellow, Bengio, Courville â€” *Deep Learning* (for NN math).  \n",
    "- Bishop â€” *Pattern Recognition and Machine Learning* (probabilistic methods).  \n",
    "- Hastie, Tibshirani, Friedman â€” *The Elements of Statistical Learning* (ML theory).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
