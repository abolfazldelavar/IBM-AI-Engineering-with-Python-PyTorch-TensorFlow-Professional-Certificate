Welcome to this video on classification. After watching this video, you will be able to: describe the classification method of supervised learning, discuss the applications and use cases of classification, list the different classification algorithms, and explain how to make multi-class predictions. Classification is a supervised machine learning, or ML method, that uses fully trained models to predict labels on new data. The labels in classification form a categorical variable with discrete values. Supervised learning aims to understand data in the correct context when answering a specific question. This ensures data accuracy when making predictions. As data is input into the model, the model adjusts the data to fit the algorithm and classifies it accordingly, defining the input and the predicted output. Classification has several applications in a wide variety of industries. Many problems can be expressed as associations between feature and target variables, particularly when labeled data is available. Classification can be used to build applications for email filtering, speech-to-text, handwriting recognition, biometric identification, document classification, and much more. Churn prediction is when you use machine learning classification to predict whether a customer will discontinue a service. Customer segmentation is when you use classification to predict the category to which a customer belongs. You can also use classification to predict whether a customer will likely respond to an advertising campaign. Let's explore additional use cases of classification. Suppose a bank is concerned that some of their loan applicants may be unable to repay their loan. The bank uses historical loan default data to predict which customers are likely to default. Here, a classifier can be trained to use customer information like age, income, and credit debt levels to learn whether they will default. Given a new customer and the same information, but without the knowledge of the likelihood of defaulting, the trained classification model predicts whether the customer is likely to default. This is an example of a binary classifier, as its predictions are limited to two possible classes. Now, consider the following example of a multi-class classifier used to help prescribe various drugs. Imagine that you collected data about a set of patients, all of whom suffered from the same illness. During their course of treatment, each patient responded positively to one of three medications. You can use this labeled dataset with a classification algorithm to build a classification model that can predict which drug might be appropriate for a future patient with the same illness. You can use many different types of algorithms to build your classification model. Some common machine learning classification algorithms include Naive Bayes, Logistic Regression, Decision Trees, K-Nearest Neighbors, Support Vector Machines, and Neural Networks. Algorithms like Logistic Regression, KNN, and Decision Trees can learn how to distinguish multiple classes. Many classification algorithms are not able to make distinctions between more than two classes, but you can use them as components for multi-class classifiers. Strategies for extending binary classifiers to handle multiple classes include one-versus-all classification and one-versus-one classification. The one-versus-all scheme implements a set of independent binary classifiers, one for each class label in the dataset. Each classifier is assigned a single label that defines its target class. Each classifier's task is to make a binary prediction for every data point about whether it has the given label, a one-versus-the-rest classifier. As you can see, if there are k classes, there will be exactly k binary classifiers contributing. Let's understand how the one-versus-all strategy decomposes a set of data points. The algorithm works on each class label, one at a time, with a trained outcome showing the points predicted to have a given color. Notice also that a given data point might not belong to any of the classes, as it might not get picked up by any of the individual classifiers. Such unclassified points fall into another class. This property might be useful for identifying outliers or noise. With the one-versus-one strategy, instead of each classifier deciding whether a point belongs to a class, the question changes from "Is it this?" to "Is it this or is it that?" Given a set of classes, consider all possible pairs of classes. For each pair of labels, a classifier is trained on the subset of the data corresponding to the two labels and decides which class each point belongs to. The process continues until all classifiers are trained. The final class label assigned to each point may be decided by a voting scheme. The simplest scheme is by popularity, meaning the class predicted by the most binary classifiers wins. Here, green is the winner. What if there is a tie? Here, we have three classes with the same number of votes. In a scenario where that is possible, it would be better to use an improved scheme, weighing each vote by the confidence level or probability assigned to that class for each classifier. Alternatively, you could try using one-versus-all classification instead. In this video, you learned that classification is a supervised ML method that uses fully trained models to predict labels on new data. Classification can be used for churn prediction, customer segmentation, and predicting advertising campaign responsiveness. Use cases of classification also include loan default prediction and multi-class drug prescription. Classification has several algorithms, which also include multi-class classifiers. Binary classifiers can be extended to handle multiple classes by using certain strategies. The one-versus-all scheme implements independent binary classifiers, one for each class label. The one-versus-one strategy answers the question, "Is it this or is it that?"