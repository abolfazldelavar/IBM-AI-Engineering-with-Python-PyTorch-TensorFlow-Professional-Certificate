Welcome to Decision Trees for Machine Learning. After watching this video, you will be able to Define Decision Trees Describe how to build Decision Trees Explain how Decision Trees learn A Decision Tree is an algorithm that can be visualized as a flowchart for classifying data points. In a Decision Tree, each internal node corresponds to a test, each branch corresponds to the result of the test, and each terminal or leaf node assigns its data to a class. A Decision Tree can be built by considering the features of a dataset one by one. Imagine that you are a researcher compiling data for a medical study. You would already have collected data about a set of patients who suffered from the same illness. During their course of treatment, each patient responded to one of two medications. Let's call them drug A and drug B. Suppose you want to build a model to predict which drug might be appropriate for a future patient with the same illness. The features of this dataset are age, gender, blood pressure, and cholesterol of our group of patients, and the target is the drug that each patient responded to. You would use the training part of the dataset to build a Decision Tree and then use it to predict the class of an unknown patient. In essence, to produce a decision on which drug the patient is likely to respond to. The decision to prescribe drug A or B will be based on historical data for a large set of patients diagnosed with the same disease. The tree starts by assigning a diagnosed patient to their age category, which can be young, middle-aged, or senior. If the patient is middle-aged, the Decision Tree suggests drug B. It also suggests drug B if the patient is young and male, or a senior with normal cholesterol. On the other hand, if the patient is a young female or a senior with high cholesterol, the tree's branches lead to a prescription for drug A. A Decision Tree is trained by growing it as follows. Start with a seed node and labeled training data. Train the node on its assigned data by finding the feature that best splits the data into its pre-labeled classes, according to a pre-selected splitting criterion. Each such split partitions the node's input data, and each partition is passed along its branch to a new node. Repeat the process for each new node, using each feature only once. The tree grows until all nodes contain a single class each, or you run out of features to select, or a pre-selected stopping criterion is met. A Decision Tree stops growing when a stopping criterion is met. This is also known as pre-emptive tree pruning. For instance, you can set stopping criteria for your model when the following criterion is met. Maximum tree depth is reached. Minimum number of data points in a node have been exceeded. Minimum number of samples in a leaf have been exceeded. Decision Tree has reached the maximum number of leaf nodes. Alternatively, you can also stop a tree from growing by cutting branches that don't significantly improve system performance. There are several reasons why you might want to prune a Decision Tree. If the tree is too complex, you might be overfitting it to the training data. If you have too many classes and features, the tree might be capturing noise and irrelevant details. Pruning simplifies your Decision Tree model and makes it amenable to generalization. A pruned tree is more concise and easier to understand. Pruning also results in better predictive accuracy. Decision Trees are built using recursive partitioning to classify the data. The Decision Tree algorithm must select a feature that best splits the data at each node to train a tree. To do this, you must select a splitting criterion to measure the split quality for determining the best split. Two common split measures are information gain, which is also called entropy reduction, and Gini impurity. Consider the 14 patients in our data set. The Decision Tree algorithm chooses the most predictive feature to split the data on, for example, the feature that best distinguishes the patient classes it assigns. Suppose it starts by testing cholesterol as the first feature to split on. The tree assigns patients to two nodes, high and normal. As you can see, if the patient has high cholesterol, we cannot say with high confidence that drug B might be suitable for him. Also, even if the patient's cholesterol is normal, we still don't have sufficient evidence or information to determine whether either drug A or drug B is suitable. Cholesterol might not be the best attribute to split on. We're looking for the best feature to decrease impurity of patients in the leaves, so let's try another feature. This time, we pick the sex feature of patients. The Decision Tree splits patients into two branches, male and female. For females, the sex split classifies most patients as drug B. For males, the distinction between drug A and B diagnoses is less clear. Further splitting the male node using the cholesterol feature results in two pure nodes: the terminal leaves in which all patients fall into a single class or prescription. The algorithm continues branching until it reaches a stopping criterion. Entropy is the measure of information disorder, or randomness in a data set. It measures how random the classes in a node are, or how uncertain the feature split result is. In Decision Trees, you look for trees that have the smallest entropy in their nodes. You can calculate the entropy of a node using the entropy formula, where pA and pB are, respectively. The proportions of drug A and drug B patients in the node. If the classes are completely homogenous, the entropy is 0, and if they are equally divided, the entropy is 1. For example, if pA equals pB equals 1 half, then the formula yields 1 half times negative 1, minus 1 half times negative 1 equals 1. You don't need to calculate these, of course, as it's calculated by the libraries or packages that you use. Information gain is the entropy of a tree before the split minus the weighted entropy after the split by a feature. For example, using cholesterol as the feature to split on for all patients yields an information gain of 0.042. You can consider information gain and entropy as opposites. As entropy decreases, the information gain, or amount of certainty, increases. Constructing a decision tree is all about finding features that return the highest information gain. Decision trees are advantageous because they can be visualized. This means you can see exactly how it makes decisions, which makes them highly interpretable. Since the tree grows by gradually selecting the next best feature to split on, you can gain insights about how important or predictive each feature is. A decision tree is an algorithm for classifying data points. Decision trees are built by considering the features of a dataset one by one. In a decision tree, each internal node corresponds to a test. Each branch corresponds to the result of the test, and each terminal, or leaf node, assigns its data to a class. In this video, you learned how to train a decision tree, how to prune a decision tree, and how to select the features that best splits the data at each node when you're training a tree. You also learned about the information gain and Gini impurity split measures. Decision trees help in visualizing a data model and predicting outcomes based on the information in a dataset.