[MUSIC] Welcome to Bias, Variance, and Ensemble Models. After watching this video, you'll be able to analyze the impact of bias and variance on accuracy and precision. You'll also be able to explain the bias-variance tradeoff in model complexity, evaluate techniques to mitigate bias and variance, and analyze the outcomes of bagging and boosting methods. Let's understand bias and variance with the four dart boards shown in the image. Closely grouping the darts near the center of the board indicates high accuracy and low bias. The top two boards demonstrate low bias, meaning they are more accurate, while the bottom two show higher bias, making them less accurate. Think of bias as how on-target or off-target the darts are. Variance measures how spread out the darts are, representing precision. The dart boards on the right display higher variance, meaning the darts are more spread out, while the boards on the left show lower variance, with the darts grouped closer together. As shown on the top left board, achieving a high score requires both low bias for accuracy and low variance for precision. Prediction bias refers to how precise a model's predictions are. It's measured by the average difference between what the model predicts and the actual target values in the data. A perfect predictor has zero bias. This chart illustrates prediction bias. The blue line represents the linear ordinary least squares fit for the blue data points. It has a bias of 0.22. The red line depicts the same model shifted down by 4 units. It has a much higher bias of 4.22. Prediction variance measures how much a model's predictions fluctuate when trained on different subsets of the same data set. When a model exhibits high prediction variance, it becomes extremely sensitive to changes in the selected training data. High variance causes the model to overfit the training data and track noise or outliers present in the training data. In contrast, models that generalize well to unseen data are necessarily less sensitive to noise. They have low prediction variance. This chart displays orange data points that follow a nonlinear pattern. Each model is fitted using a randomly sampled training data set. The curves would align almost perfectly if the prediction variance were near zero. However, you can observe differences between the curves, especially at the beginning and end of the data. This variation indicates some prediction variance, reflecting instability in the model's predictions. This plot illustrates how bias and variance change as your model becomes more complex and better at predicting the data it's trained on. As model complexity increases, bias, represented by the blue curve, tends to decline while variance, shown by the green curve, rises. When model complexity is low, bias is high, leading to poor predictions even on training data. This is known as underfitting. Conversely, high model complexity results in high variance, meaning the model becomes overly sensitive to the training data and performs poorly on unseen data, resulting in overfitting. However, there's a crossover point marked by the vertical dashed line where the model's complexity is just right. As the plot indicates, there will always be some generalization error that cannot be eliminated, such as random noise in the data. A weak learner is a supervised machine learning model that performs only slightly better than random guessing. These models are characterized by high bias and low variance, which often leads to underfitting. In contrast, strong learners exhibit low bias and high variance, resulting in a tendency to overfit the data. Bagging and boosting are well-known ensemble methods that effectively balance bias and variance. Decision or regression trees are commonly chosen as base learners in ensemble learning because their bias and variance can be easily adjusted by altering their depth. The model predictions shown here utilize the same modeling algorithm, repeatedly trained on bootstrapped subsets of data. You can observe the variance at both ends of the family of curves. Now, imagine if you were to perform this process multiple times and then average the predictions. This technique is known as bagging or bootstrap aggregating. As illustrated by the dashed curve, averaging the models across numerous iterations significantly reduces prediction variance while also lowering the risk of overfitting. Random forests is a bagging method that trains multiple decision trees on bootstrapped data sets. These trees don't need to be very deep. Instead, the focus should be on minimizing prediction bias. Shallow trees have high prediction variance, and aggregation significantly reduces this variance while only slightly increasing bias. Boosting is an ensemble modeling technique that builds a series of weak learners, each aimed at correcting the errors of the previous one. By systematically reducing prediction error, boosting helps lower prediction bias. The final model is formed as a weighted sum of these weak learners. In each iteration of the process, the weights of misclassified data from the previous model are increased, while the weights of correctly classified data are decreased. This reweighting helps the algorithm focus on correcting the mistakes. The model's weights are updated based on the performance of each weak learner. Popular boosting algorithms include Gradient Boosting, XGBoost, and AdaBoost. This graph demonstrates how bagging and boosting can help mitigate the bias-variance tradeoff by strategically adjusting model complexity. Boosting increases model complexity and decreases bias. In contrast, bagging reduces variance. This table illustrates how ensemble methods can be used to address common issues in machine learning. Bagging aims to mitigate overfitting by combining multiple base learnings that are high variance and low bias. These base learners are trained in parallel on bootstrapped data samples. Bagging helps reduce variance. Boosting aims to mitigate underfitting by sequentially training base learners that are low variance and high bias. Each subsequent base learner builds on the previous result, gradually reducing bias. In this video, you learned to analyze bias and variance and how they impact accuracy and precision. Explain prediction bias and how it measures the accuracy of predictions. Analyze prediction variance to measure how much a model's predictions fluctuate. Explain the bias-variance tradeoff and how bias and variance change as your model becomes more complex. Explain mitigating bias and variance and the concept of weak and strong learners. Analyze bagging or bootstrap aggregating to observe variance at both ends of a family of curves. Explain random forests to train multiple decision trees on bootstrap data sets. And finally, analyze bagging and boosting outcomes to manage bias and variance. [MUSIC]