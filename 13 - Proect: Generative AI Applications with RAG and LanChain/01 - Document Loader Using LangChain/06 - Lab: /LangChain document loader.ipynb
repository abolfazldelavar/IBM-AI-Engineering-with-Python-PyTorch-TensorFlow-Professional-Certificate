{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53d5dc6-b328-4c41-9a3d-c9ebab1f2a7f",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d34519f-7b0a-470a-8573-405a9088b86d",
   "metadata": {},
   "source": [
    "# Load Documents Using LangChain for Different Sources \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e20f6-b9f6-45b5-99fa-16967833214b",
   "metadata": {},
   "source": [
    "Estimated time needed: **20** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc8166-cebb-4aea-9c49-d0e5d195edb0",
   "metadata": {},
   "source": [
    "Imagine you are working as a data scientist at a consulting firm, and you've been tasked with analyzing documents from multiple clients. Each client provides their data in different formats: some in PDFs, others in Word documents, CSV files, or even HTML webpages. Manually loading and parsing each document type is not only time-consuming but also prone to errors. Your goal is to streamline this process, making it efficient and error-free.\n",
    "\n",
    "To achieve this, you'll use LangChain’s powerful document loaders. These loaders allow you to read and convert various file formats into a unified document structure that can be easily processed. For example, you'll load client policy documents from text files, financial reports from PDFs, marketing strategies from Word documents, and product reviews from JSON files. By the end of this lab, you will have a robust pipeline that can handle any new file formats clients might send, saving you valuable time and effort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851f784-c7c0-4575-a998-b71cb96097d0",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Hvf-jk8b5Fs-E_E4AJyEow/loader.png\" width=\"50%\" alt=\"indexing\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cf0f8-104e-47e7-b12c-4bbea304cd12",
   "metadata": {},
   "source": [
    "In this lab, you will explore how to use various loaders provided by LangChain to load and process data from different file formats. These loaders simplify the task of reading and converting files into a document format that can be processed downstream. By the end of this lab, you will be able to efficiently load text, PDF, Markdown, JSON, CSV, DOCX, and other file formats into a unified format, allowing for seamless data analysis and manipulation for LLM applications.\n",
    "\n",
    "(Note: In this lab, we just introduced several commonly used file format loaders. LangChain provides more document loaders for various document formats [here](https://python.langchain.com/v0.2/docs/integrations/document_loaders/).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc440c50-b18e-4910-87d6-5123f505bc34",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Load-from-TXT-files\">Load from TXT files</a></li>\n",
    "    <li><a href=\"#Load-from-PDF-files\">Load from PDF files</a></li>\n",
    "    <li><a href=\"#Load-from-Markdown-files\">Load from Markdown files</a></li>\n",
    "    <li><a href=\"#Load-from-JSON-files\">Load from JSON files</a></li>\n",
    "    <li><a href=\"#Load-from-CSV-files\">Load from CSV files</a></li>\n",
    "    <li><a href=\"#Load-from-URL/Website-files\">Load from URL/Webpage files</a></li>\n",
    "    <li><a href=\"#Load-from-WORD-files\">Load from WORD files</a></li>\n",
    "    <li><a href=\"#Load-from-Unstructured-Files\">Load from Unstructured Files</a></li>\n",
    "</ol>\n",
    "\n",
    "<a href=\"#Exercises\">Exercises</a>\n",
    "<ol>\n",
    "    <li><a href=\"#Exercise-1---Try-to-use-other-PDF-loaders\">Exercise 1 - Try to use other PDF loaders</a></li>\n",
    "    <li><a href=\"#Exercise-2---Load-from-Arxiv\">Exercise 2 - Load from Arxiv</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a732cb4-cd6c-483c-9009-252b8f264649",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Understand how to use `TextLoader` to load text files.\n",
    " - Learn how to load PDFs using `PyPDFLoader` and `PyMuPDFLoader`.\n",
    " - Use `UnstructuredMarkdownLoader` to load Markdown files.\n",
    " - Load JSON files with `JSONLoader` using jq schemas.\n",
    " - Process CSV files with `CSVLoader` and `UnstructuredCSVLoader`.\n",
    " - Load Webpage content using `WebBaseLoader`.\n",
    " - Load Word documents using `Docx2txtLoader`.\n",
    " - Utilize `UnstructuredFileLoader` for various file types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac77ac-0d54-41f0-95ad-105875f9846f",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0b27b-87ae-47d7-a91f-1e1bc9e42bf4",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e997f3f-ecd2-4141-8252-9a12e4ff9180",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are __not__ preinstalled in the Skills Network Labs environment. __You must run the following cell__ to install them:\n",
    "\n",
    "**Note:** We are pinning the version here to specify the version. It's recommended that you do this as well. Even if the library is updated in the future, the installed library could still support this lab work.\n",
    "\n",
    "This might take approximately 1 minute. \n",
    "\n",
    "As we use `%%capture` to capture the installation, you won't see the output process. But after the installation completes, you will see a number beside the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3634d8b-2f4b-413c-982e-0f885875e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "#After executing the cell,please RESTART the kernel and run all the cells.\n",
    "!pip install --user \"langchain-community==0.2.1\"\n",
    "!pip install --user \"pypdf==4.2.0\"\n",
    "!pip install --user \"PyMuPDF==1.24.5\"\n",
    "!pip install --user \"unstructured==0.14.8\"\n",
    "!pip install --user \"markdown==3.6\"\n",
    "!pip install --user  \"jq==1.7.0\"\n",
    "!pip install --user \"pandas==2.2.2\"\n",
    "!pip install --user \"docx2txt==0.8\"\n",
    "!pip install --user \"requests==2.32.3\"\n",
    "!pip install --user \"beautifulsoup4==4.12.3\"\n",
    "!pip install --user \"nltk==3.8.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/abolfazl/.local/lib/python3.10/site-packages (3.8)\n",
      "Requirement already satisfied: click in /home/abolfazl/.local/lib/python3.10/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /home/abolfazl/anaconda3/envs/myenv/lib/python3.10/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/abolfazl/.local/lib/python3.10/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/abolfazl/anaconda3/envs/myenv/lib/python3.10/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd3530-7698-4c3f-8c94-57fcf654df39",
   "metadata": {},
   "source": [
    "After you install the libraries, restart your kernel. You can do that by clicking the **Restart the kernel** icon.\n",
    "\n",
    "<p style=\"text-align:left\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/1_Bd_EvpEzLH9BbxRXXUGQ/screenshot-to-replace.png\" width=\"50%\"/>\n",
    "    </a>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657fa8d-7bf0-4a5f-bbcb-0fade410b554",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0f6527-66f2-43da-8493-46988fa0d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/abolfazl/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/abolfazl/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders.csv_loader import UnstructuredCSVLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92947d6-5c23-4890-a5c4-31bd95af73d3",
   "metadata": {},
   "source": [
    "### Load from TXT files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82e20f-1d06-4d79-9271-170a6f05fe58",
   "metadata": {},
   "source": [
    "The `TextLoader` is a tool designed to load textual data from various sources.\n",
    "\n",
    "It is the simplest loader, reading a file as text and placing all the content into a single document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b784f-d2b2-4b11-8dc5-26b75cb37889",
   "metadata": {},
   "source": [
    "We have prepared a .txt file for you to load. First, we need to download it from a remote source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e1897-e31a-4290-aba3-aaacd698781d",
   "metadata": {},
   "source": [
    "We have prepared a .txt file for you to load. First, we need to download it from a remote source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6a14d8-b09a-43c9-b6b8-128b42737df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-23 21:26:15--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6363 (6.2K) [text/plain]\n",
      "Saving to: ‘new-Policies.txt’\n",
      "\n",
      "new-Policies.txt    100%[===================>]   6.21K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-11-23 21:26:17 (70.4 MB/s) - ‘new-Policies.txt’ saved [6363/6363]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18ad4e-feda-478b-a125-3d5fa988e056",
   "metadata": {},
   "source": [
    "Next, we will use the `TextLoader` class to load the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ec87ba-56fe-4752-8a1f-4fd256d690c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x7ebf189f1660>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"new-Policies.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e53a2e-9f47-4ce3-af41-945bc6c5c816",
   "metadata": {},
   "source": [
    "Here, we use the `load` method to load the data as documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20131104-feab-4ebd-b328-690b010ce2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478fd12-f931-47e9-8050-4b659a68f38d",
   "metadata": {},
   "source": [
    "Let's present the entire data (document) here.\n",
    "\n",
    "This is a `document` object that includes `page_content` and `metadata` attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00765b13-1809-476f-8f54-a6042e2786c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content=\"1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual's contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\\n\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5184e3c-1403-41df-b724-04c2b677baed",
   "metadata": {},
   "source": [
    "We can also use the `pprint` function to print the first 1000 characters of the `page_content` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d2d1e47-803f-4da6-863a-588b8e67b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1. Code of Conduct\\n'\n",
      " '\\n'\n",
      " 'Our Code of Conduct establishes the core values and ethical standards that '\n",
      " 'all members of our organization must adhere to. We are committed to '\n",
      " 'fostering a workplace characterized by integrity, respect, and '\n",
      " 'accountability.\\n'\n",
      " '\\n'\n",
      " 'Integrity: We commit to the highest ethical standards by being honest and '\n",
      " 'transparent in all our dealings, whether with colleagues, clients, or the '\n",
      " 'community. We protect sensitive information and avoid conflicts of '\n",
      " 'interest.\\n'\n",
      " '\\n'\n",
      " \"Respect: We value diversity and every individual's contribution. \"\n",
      " 'Discrimination, harassment, or any form of disrespect is not tolerated. We '\n",
      " 'promote an inclusive environment where differences are respected, and '\n",
      " 'everyone is treated with dignity.\\n'\n",
      " '\\n'\n",
      " 'Accountability: We are responsible for our actions and decisions, complying '\n",
      " 'with all relevant laws and regulations. We aim for continuous improvement '\n",
      " 'and report any breaches of this code, supporting investigations into such '\n",
      " 'matters.\\n'\n",
      " '\\n'\n",
      " 'Safety: We prioritize the safety of our employees, c')\n"
     ]
    }
   ],
   "source": [
    "pprint(data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200923f4-7038-40ad-8506-ca303cfef837",
   "metadata": {},
   "source": [
    "### Load from PDF files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca89ed-f3c1-4177-b14e-3dbbb89c691d",
   "metadata": {},
   "source": [
    "Sometimes, we may have files in PDF format that we want to load for processing.\n",
    "\n",
    "LangChain provides several classes for loading PDFs. Here, we introduce two classes: `PyPDFLoader` and `PyMuPDFLoader`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eba7a4-9a96-432d-b66c-f6983ff1f802",
   "metadata": {},
   "source": [
    "#### PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844d54a-1bb3-4021-a340-6a19d4fa39ca",
   "metadata": {},
   "source": [
    "Load the PDF using `PyPDFLoader` into an array of documents, where each document contains the page content and metadata with the page number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beb2ad22-652a-482d-9405-5f063e409ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_url)\n",
    "\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b3f20-3dcf-4afd-af8f-b364245c1178",
   "metadata": {},
   "source": [
    "Display the first page of the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7b7282a-e9bc-4a3c-b5a6-cf8df73d3bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LAB: L ARGE -SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1 I NTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines:{News article}) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1\n",
      "arXiv:2403.01081v3  [cs.CL]  29 Apr 2024' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932cc612-bade-43e0-8fc6-a13fd2d90748",
   "metadata": {},
   "source": [
    "Display the first three pages of the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fafa7292-b1d6-4fd3-be2b-15f401ec4904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page number 1\n",
      "page_content='LAB: L ARGE -SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1 I NTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines:{News article}) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1\n",
      "arXiv:2403.01081v3  [cs.CL]  29 Apr 2024' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}\n",
      "page number 2\n",
      "page_content='training captures enough of the distribution of language and knowledge, such that a small amount of\n",
      "supervised training can “unlock” or shape latent abilities related to the ultimate desired instruction-\n",
      "following behavior of the model. However, unlike the unstructured data that is abundantly available\n",
      "in the public domain, high-quality, human-generated task-specific instruction data is costly to pro-\n",
      "cure, even via crowd-sourcing, and human-generated instruction data is typically closely guarded\n",
      "by model builders, even for ostensibly “open” model-building efforts. In this work, we address\n",
      "the challenges associated with scaling of the alignment-tuning phase and propose a new method\n",
      "called LAB: Large-scale Alignment for chatBots. The LAB method consists of two components:\n",
      "(i) a taxonomy-guided synthetic data generation method and quality assurance process that yields a\n",
      "highly diverse and high-quality instruction dataset, without resorting to the use of proprietary LLMs\n",
      "like GPT-4 or substantial human curation, and (ii) a novel multi-phase training framework and un-\n",
      "conventional tuning regime that allows for adding new knowledge and instruction-following abilities\n",
      "into pre-trained LLMs without suffering from catastrophic forgetting. Our findings show that LAB-\n",
      "trained models can perform competitively with proprietary and open-source models that use human\n",
      "annotations and/or synthetic data generated using GPT-4 on a number of benchmarks.\n",
      "2 R ELATED WORK\n",
      "Existing methods for instruction tuning typically either rely on humans for generating high-quality\n",
      "datasets, or use synthetic data generation using a large teacher model. OpenAI (Ouyang et al.,\n",
      "2022) arguably set the standard for model alignment from human data, employing human annota-\n",
      "tors to gather data for supervised fine tuning (SFT) and reinforcement learning with human feed-\n",
      "back (RLHF) training. Collecting human-generated data for these steps is complex undertaking; the\n",
      "selection of annotators requires a rigorous multi-stage screening process aimed at achieving high\n",
      "inter-annotator agreement, and collecting even modest amounts data (by LLM standards) requires\n",
      "the coordination of large groups of annotators. The creators of the LLaMA 2 model series (Touvron\n",
      "et al., 2023) followed a similar recipe, collecting tens of thousands of human-generated instruction\n",
      "samples, and approximately 1 million human-annotated binary comparisons for reward modeling.\n",
      "Not only are such approaches expensive and time consuming, but they can also potentially limit\n",
      "agility in exploring the space of instructions and capabilities the model is trained to perform. Alter-\n",
      "natives to this approach, such as transforming existing human datasets into instructions via templat-\n",
      "ing (Wei et al.) can be more cost effective, but face limitations in the naturalness and length of the\n",
      "responses used for training.\n",
      "More recently, training with synthetic data generated from LLMs has emerged as an alternative to\n",
      "purely human-data-based approaches. Wang et al. (2023) introduced Self-Instruct, which leverages\n",
      "a small number of handwritten human seed instructions as input to bootstrapping process to generate\n",
      "a large number of samples using an LLM’s own generation abilities. Taori et al. (2023) built upon\n",
      "Self-Instruct, using a larger teacher model to generate synthetic data to train a smaller student model,\n",
      "and incorporating principles in the generation prompt to promote diversity in the generated instruc-\n",
      "tion data. Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes\n",
      "iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee\n",
      "et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task di-\n",
      "versity and scalability, alongside a progressive training framework aimed at improving the model’s\n",
      "reasoning ability and response style to match teacher models. This is achieved by generating rich' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 1}\n",
      "page number 3\n",
      "page_content='versity and scalability, alongside a progressive training framework aimed at improving the model’s\n",
      "reasoning ability and response style to match teacher models. This is achieved by generating rich\n",
      "reasoning signals in the generated answer and progressively training on datasets of varying difficulty\n",
      "in incremental phases.\n",
      "Similar to LAB, concurrent work, GLAN (Li et al., 2024), employs a semi-automatic approach to\n",
      "synthetic data generation that uses a human-curated taxonomy to generate instruction tuning data\n",
      "from a teacher model. However, as explained in section 3.2.2, unlike LAB, GLAN cannot be used\n",
      "to generate synthetic data from domains that are not captured in the teacher model’s support. As\n",
      "such, while LAB uses the open-source Mixtral model as the teacher, like many other synthetic\n",
      "data generation approaches, GLAN has to rely on a large proprietary model (GPT-4). This poses\n",
      "complicated questions about the usability of generated data (especially for commercial purposes)\n",
      "since the terms of use of proprietary models typically forbid using the model to improve other\n",
      "models.\n",
      "2' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "for p,page in enumerate(pages[0:3]):\n",
    "    print(f\"page number {p+1}\")\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cabbeb-4ae0-41f5-8be6-4ae7126ec23f",
   "metadata": {},
   "source": [
    "#### PyMuPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9c134-bd83-4027-a495-45b9997752b6",
   "metadata": {},
   "source": [
    "`PyMuPDFLoader` is the fastest of the PDF parsing options. It provides detailed metadata about the PDF and its pages, and returns one document per page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bc230ed-ccd6-427a-886e-67e5e9293a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyMuPDFLoader at 0x7ebee03a2e60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyMuPDFLoader(pdf_url)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db2132a5-0c7c-4808-afe5-7c5bbc468905",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5daf28f-502f-49b1-8973-dff353af2522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LAB: LARGE-SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1\n",
      "INTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines: {News article}) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1\n",
      "arXiv:2403.01081v3  [cs.CL]  29 Apr 2024\n",
      "' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0, 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240501000524Z', 'modDate': 'D:20240501000524Z', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d1d2a8-36ec-4627-b660-4864a2ffd3a1",
   "metadata": {},
   "source": [
    "The `metadata` attribute reveals that `PyMuPDFLoader` provides more detailed metadata information than `PyPDFLoader`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd83e8-7c02-47a6-82ee-090135fd4a06",
   "metadata": {},
   "source": [
    "### Load from Markdown files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f0450-5c14-4e93-bdcb-78c35de75e24",
   "metadata": {},
   "source": [
    "Sometimes, our file source might be in Markdown format.\n",
    "\n",
    "LangChain provides the `UnstructuredMarkdownLoader` to load content from Markdown files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "404c460f-9375-4ca5-b3f0-fd93de480c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-23 21:27:11--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eMSP5vJjj9yOfAacLZRWsg/markdown-sample.md\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 198.23.119.245\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|198.23.119.245|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3398 (3.3K) [text/markdown]\n",
      "Saving to: ‘markdown-sample.md’\n",
      "\n",
      "markdown-sample.md  100%[===================>]   3.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-11-23 21:27:12 (337 MB/s) - ‘markdown-sample.md’ saved [3398/3398]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eMSP5vJjj9yOfAacLZRWsg/markdown-sample.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84c0d0c4-b8b7-4235-9a52-d8c85c732642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader at 0x7ebee03a2e90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_path = \"markdown-sample.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93f305ae-dcf0-4411-93f0-0ed64900fe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/abolfazl/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/abolfazl/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f913713d-52cc-4169-a4e8-c99e7fecc96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'markdown-sample.md'}, page_content='An h1 header\\n\\nParagraphs are separated by a blank line.\\n\\n2nd paragraph. Italic, bold, and monospace. Itemized lists\\nlook like:\\n\\nthis one\\n\\nthat one\\n\\nthe other one\\n\\nNote that --- not considering the asterisk --- the actual text\\ncontent starts at 4-columns in.\\n\\nBlock quotes are\\nwritten like so.\\n\\nThey can span multiple paragraphs,\\nif you like.\\n\\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all\\nin chapters 12--14\"). Three dots ... will be converted to an ellipsis.\\nUnicode is supported. ☺\\n\\nAn h2 header\\n\\nHere\\'s a numbered list:\\n\\nfirst item\\n\\nsecond item\\n\\nthird item\\n\\nNote again how the actual text starts at 4 columns in (4 characters\\nfrom the left side). Here\\'s a code sample:\\n\\nAs you probably guessed, indented 4 spaces. By the way, instead of\\nindenting the block, you can use delimited blocks, if you like:\\n\\n~~~\\ndefine foobar() {\\n    print \"Welcome to flavor country!\";\\n}\\n~~~\\n\\n(which makes copying & pasting easier). You can optionally mark the\\ndelimited block for Pandoc to syntax highlight it:\\n\\n~~~python\\nimport time\\n\\nQuick, count to ten!\\n\\nfor i in range(10):\\n    # (but not too quick)\\n    time.sleep(0.5)\\n    print i\\n~~~\\n\\nAn h3 header\\n\\nNow a nested list:\\n\\nFirst, get these ingredients:\\n\\ncarrots\\ncelery\\nlentils\\n\\nBoil some water.\\n\\nDump everything in the pot and follow\\n    this algorithm:\\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)\\n\\nDo not bump wooden spoon or it will fall.\\n\\nNotice again how text always lines up on 4-space indents (including\\nthat last line which continues item 3 above).\\n\\nHere\\'s a link to a website, to a local\\ndoc, and to a section heading in the current\\ndoc. Here\\'s a footnote [^1].\\n\\n[^1]: Footnote text goes here.\\n\\nTables can look like this:\\n\\nsize  material      color\\n\\n9     leather       brown\\n10    hemp canvas   natural\\n11    glass         transparent\\n\\nTable: Shoes, their sizes, and what they\\'re made of\\n\\n(The above is the caption for the table.) Pandoc also supports\\nmulti-line tables:\\n\\nkeyword   text\\n\\nred       Sunsets, apples, and\\n          other red or reddish\\n          things.\\n\\ngreen     Leaves, grass, frogs\\n          and other things it\\'s\\n          not easy being.\\n\\nA horizontal rule follows.\\n\\nHere\\'s a definition list:\\n\\napples\\n  : Good for making applesauce.\\noranges\\n  : Citrus!\\ntomatoes\\n  : There\\'s no \"e\" in tomatoe.\\n\\nAgain, text is indented 4 spaces. (Put a blank line between each\\nterm/definition pair to spread things out more.)\\n\\nHere\\'s a \"line block\":\\n\\n| Line one\\n|   Line too\\n| Line tree\\n\\nand images can be specified like so:\\n\\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display\\nmath should get its own line and be put in in double-dollarsigns:\\n\\n$$I = \\\\int \\\\rho R^{2} dV$$\\n\\nAnd note that you can backslash-escape any punctuation characters\\nwhich you wish to be displayed literally, ex.: `foo`, *bar*, etc.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a513fd-100d-4808-863c-0000e2beb7f1",
   "metadata": {},
   "source": [
    "### Load from JSON files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1644c-7c54-4b69-a112-1e95cb7be0bd",
   "metadata": {},
   "source": [
    "The JSONLoader uses a specified [jq schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files. It uses the jq python package, which we've installed before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76881f81-27cc-4593-8c00-8de9b499ea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-23 21:27:54--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hAmzVJeOUAMHzmhUHNdAUg/facebook-chat.json\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 198.23.119.245\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|198.23.119.245|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2167 (2.1K) [application/json]\n",
      "Saving to: ‘facebook-chat.json’\n",
      "\n",
      "facebook-chat.json  100%[===================>]   2.12K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-11-23 21:27:56 (249 MB/s) - ‘facebook-chat.json’ saved [2167/2167]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hAmzVJeOUAMHzmhUHNdAUg/facebook-chat.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5426ae-86ec-4c42-99ee-7201652e6347",
   "metadata": {},
   "source": [
    "First, let's use `pprint` to take a look at the JSON file and its structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f481da92-4c5f-4ddb-bc38-27733c2c1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='facebook-chat.json'\n",
    "data = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6787a446-d4ce-404b-a22d-5d88c8d6decb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},\n",
      " 'is_still_participant': True,\n",
      " 'joinable_mode': {'link': '', 'mode': 1},\n",
      " 'magic_words': [],\n",
      " 'messages': [{'content': 'Bye!',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675597571851},\n",
      "              {'content': 'Oh no worries! Bye',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675597435669},\n",
      "              {'content': 'No Im sorry it was my mistake, the blue one is not '\n",
      "                          'for sale',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675596277579},\n",
      "              {'content': 'I thought you were selling the blue one!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675595140251},\n",
      "              {'content': 'Im not interested in this bag. Im interested in the '\n",
      "                          'blue one!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675595109305},\n",
      "              {'content': 'Here is $129',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595068468},\n",
      "              {'photos': [{'creation_timestamp': 1675595059,\n",
      "                           'uri': 'url_of_some_picture.jpg'}],\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595060730},\n",
      "              {'content': 'Online is at least $100',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595045152},\n",
      "              {'content': 'How much do you want?',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675594799696},\n",
      "              {'content': 'Goodmorning! $50 is too low.',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675577876645},\n",
      "              {'content': 'Hi! Im interested in your bag. Im offering $50. Let '\n",
      "                          'me know if you are interested. Thanks!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675549022673}],\n",
      " 'participants': [{'name': 'User 1'}, {'name': 'User 2'}],\n",
      " 'thread_path': 'inbox/User 1 and User 2 chat',\n",
      " 'title': 'User 1 and User 2 chat'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ca00c-2961-47c7-82c4-28991575ab56",
   "metadata": {},
   "source": [
    "We use `JSONLoader` to load data from the JSON file. However, JSON files can have various attribute-value pairs. If we want to load a specific attribute and its value, we need to set an appropriate `jq schema`.\n",
    "\n",
    "So for example, if we want to load the `content` from the JSON file, we need to set `jq_schema='.messages[].content'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40e362ba-9654-43e0-80d6-b871c91603f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "100555fd-c1f9-4e32-b63e-12cee8ef8ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/home/abolfazl/Documents/IBM-AI-Engineering-with-Python-PyTorch-TensorFlow-Professional-Certificate/13 - Proect: Generative AI Applications with RAG and LanChain/01 - Document Loader Using LangChain/06 - Lab: /facebook-chat.json', 'seq_num': 1}, page_content='Bye!'),\n",
      " Document(metadata={'source': '/home/abolfazl/Documents/IBM-AI-Engineering-with-Python-PyTorch-TensorFlow-Professional-Certificate/13 - Proect: Generative AI Applications with RAG and LanChain/01 - Document Loader Using LangChain/06 - Lab: /facebook-chat.json', 'seq_num': 2}, page_content='Oh no worries! Bye'),\n",
      " Document(metadata={'source': '/home/abolfazl/Documents/IBM-AI-Engineering-with-Python-PyTorch-TensorFlow-Professional-Certificate/13 - Proect: Generative AI Applications with RAG and LanChain/01 - Document Loader Using LangChain/06 - Lab: /facebook-chat.json', 'seq_num': 3}, page_content='No Im sorry it was my mistake, the blue one is not for sale'),\n",
      " Document(metadata={'source': '/home/abolfazl/Documents/IBM-AI-Engineering-with-Python-PyTorch-TensorFlow-Professional-Certificate/13 - Proect: Generative AI Applications with RAG and LanChain/01 - Document Loader Using LangChain/06 - Lab: /facebook-chat.json', 'seq_num': 4}, page_content='I thought you were selling the blue one!'),\n",
      " Document(metadata={'source': '/home/abolfazl/Documents/IBM-AI-Engineering-with-Python-PyTorch-TensorFlow-Professional-Certificate/13 - Proect: Generative AI Applications with RAG and LanChain/01 - Document Loader Using LangChain/06 - Lab: /facebook-chat.json', 'seq_num': 5}, page_content='Im not interested in this bag. Im interested in the blue one!'),\n",
      " Document(metadata={'source': '/home/abolfazl/Documents/IBM-AI-Engineering-with-Python-PyTorch-TensorFlow-Professional-Certificate/13 - Proect: Generative AI Applications with RAG and LanChain/01 - Document Loader Using LangChain/06 - Lab: /facebook-chat.json', 'seq_num': 6}, page_content='Here is $129'),\n",
      " Document(metadata={'source': '/home/abolfazl/Documents/IBM-AI-Engineering-with-Python-PyTorch-TensorFlow-Professional-Certificate/13 - Proect: Generative AI Applications with RAG and LanChain/01 - Document Loader Using LangChain/06 - Lab: /facebook-chat.json', 'seq_num': 7}, page_content=''),\n",
      " Document(metadata={'source': '/home/abolfazl/Documents/IBM-AI-Engineering-with-Python-PyTorch-TensorFlow-Professional-Certificate/13 - Proect: Generative AI Applications with RAG and LanChain/01 - Document Loader Using LangChain/06 - Lab: /facebook-chat.json', 'seq_num': 8}, page_content='Online is at least $100'),\n",
      " Document(metadata={'source': '/home/abolfazl/Documents/IBM-AI-Engineering-with-Python-PyTorch-TensorFlow-Professional-Certificate/13 - Proect: Generative AI Applications with RAG and LanChain/01 - Document Loader Using LangChain/06 - Lab: /facebook-chat.json', 'seq_num': 9}, page_content='How much do you want?'),\n",
      " Document(metadata={'source': '/home/abolfazl/Documents/IBM-AI-Engineering-with-Python-PyTorch-TensorFlow-Professional-Certificate/13 - Proect: Generative AI Applications with RAG and LanChain/01 - Document Loader Using LangChain/06 - Lab: /facebook-chat.json', 'seq_num': 10}, page_content='Goodmorning! $50 is too low.'),\n",
      " Document(metadata={'source': '/home/abolfazl/Documents/IBM-AI-Engineering-with-Python-PyTorch-TensorFlow-Professional-Certificate/13 - Proect: Generative AI Applications with RAG and LanChain/01 - Document Loader Using LangChain/06 - Lab: /facebook-chat.json', 'seq_num': 11}, page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!')]\n"
     ]
    }
   ],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b083a0-8185-4a74-9811-fbfb65a99411",
   "metadata": {},
   "source": [
    "### Load from CSV files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341a278-469d-4d6f-a8ff-0591bb004073",
   "metadata": {},
   "source": [
    "CSV files are a common format for storing tabular data. The `CSVLoader` provides a convenient way to read and process this data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9dca2a0-4b73-4c94-a08c-48d5014da5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-23 21:28:09--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IygVG_j0M87BM4Z0zFsBMA/mlb-teams-2012.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 848 [text/csv]\n",
      "Saving to: ‘mlb-teams-2012.csv’\n",
      "\n",
      "mlb-teams-2012.csv  100%[===================>]     848  --.-KB/s    in 0s      \n",
      "\n",
      "2025-11-23 21:28:11 (168 MB/s) - ‘mlb-teams-2012.csv’ saved [848/848]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IygVG_j0M87BM4Z0zFsBMA/mlb-teams-2012.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d7a0744-c6b3-4ef8-a39c-461589024869",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='mlb-teams-2012.csv')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1cca688-5f2e-4c87-9164-cbaff3e02b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 0}, page_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 1}, page_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 2}, page_content='Team: Yankees\\n\"Payroll (millions)\": 197.96\\n\"Wins\": 95'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 3}, page_content='Team: Giants\\n\"Payroll (millions)\": 117.62\\n\"Wins\": 94'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 4}, page_content='Team: Braves\\n\"Payroll (millions)\": 83.31\\n\"Wins\": 94'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 5}, page_content='Team: Athletics\\n\"Payroll (millions)\": 55.37\\n\"Wins\": 94'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 6}, page_content='Team: Rangers\\n\"Payroll (millions)\": 120.51\\n\"Wins\": 93'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 7}, page_content='Team: Orioles\\n\"Payroll (millions)\": 81.43\\n\"Wins\": 93'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 8}, page_content='Team: Rays\\n\"Payroll (millions)\": 64.17\\n\"Wins\": 90'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 9}, page_content='Team: Angels\\n\"Payroll (millions)\": 154.49\\n\"Wins\": 89'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 10}, page_content='Team: Tigers\\n\"Payroll (millions)\": 132.30\\n\"Wins\": 88'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 11}, page_content='Team: Cardinals\\n\"Payroll (millions)\": 110.30\\n\"Wins\": 88'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 12}, page_content='Team: Dodgers\\n\"Payroll (millions)\": 95.14\\n\"Wins\": 86'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 13}, page_content='Team: White Sox\\n\"Payroll (millions)\": 96.92\\n\"Wins\": 85'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 14}, page_content='Team: Brewers\\n\"Payroll (millions)\": 97.65\\n\"Wins\": 83'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 15}, page_content='Team: Phillies\\n\"Payroll (millions)\": 174.54\\n\"Wins\": 81'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 16}, page_content='Team: Diamondbacks\\n\"Payroll (millions)\": 74.28\\n\"Wins\": 81'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 17}, page_content='Team: Pirates\\n\"Payroll (millions)\": 63.43\\n\"Wins\": 79'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 18}, page_content='Team: Padres\\n\"Payroll (millions)\": 55.24\\n\"Wins\": 76'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 19}, page_content='Team: Mariners\\n\"Payroll (millions)\": 81.97\\n\"Wins\": 75'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 20}, page_content='Team: Mets\\n\"Payroll (millions)\": 93.35\\n\"Wins\": 74'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 21}, page_content='Team: Blue Jays\\n\"Payroll (millions)\": 75.48\\n\"Wins\": 73'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 22}, page_content='Team: Royals\\n\"Payroll (millions)\": 60.91\\n\"Wins\": 72'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 23}, page_content='Team: Marlins\\n\"Payroll (millions)\": 118.07\\n\"Wins\": 69'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 24}, page_content='Team: Red Sox\\n\"Payroll (millions)\": 173.18\\n\"Wins\": 69'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 25}, page_content='Team: Indians\\n\"Payroll (millions)\": 78.43\\n\"Wins\": 68'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 26}, page_content='Team: Twins\\n\"Payroll (millions)\": 94.08\\n\"Wins\": 66'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 27}, page_content='Team: Rockies\\n\"Payroll (millions)\": 78.06\\n\"Wins\": 64'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 28}, page_content='Team: Cubs\\n\"Payroll (millions)\": 88.19\\n\"Wins\": 61'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 29}, page_content='Team: Astros\\n\"Payroll (millions)\": 60.65\\n\"Wins\": 55')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eedc775-4081-4e7d-bb69-a410b23d8d83",
   "metadata": {},
   "source": [
    "When you load data from a CSV file, the loader typically creates a separate `Document` object for each row of data in the CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691f20f-d5c5-4f7b-9732-d28fd69706bb",
   "metadata": {},
   "source": [
    "#### UnstructuredCSVLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a7ba9-a6ae-4384-b6c7-4ede9fada07a",
   "metadata": {},
   "source": [
    "In contrast to `CSVLoader`, which treats each row as an individual document with headers defining the data, `UnstructuredCSVLoader` considers the entire CSV file as a single unstructured table element. This approach is beneficial when you want to analyze the data as a complete table rather than as separate entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b962fce-1a32-44f2-a8fb-051bd1766a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredCSVLoader(\n",
    "    file_path=\"mlb-teams-2012.csv\", mode=\"elements\"\n",
    ")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db855c95-bcf4-43d2-9135-7f13af879319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nTeam\\n\"Payroll (millions)\"\\n\"Wins\"\\n\\n\\nNationals\\n81.34\\n98\\n\\n\\nReds\\n82.20\\n97\\n\\n\\nYankees\\n197.96\\n95\\n\\n\\nGiants\\n117.62\\n94\\n\\n\\nBraves\\n83.31\\n94\\n\\n\\nAthletics\\n55.37\\n94\\n\\n\\nRangers\\n120.51\\n93\\n\\n\\nOrioles\\n81.43\\n93\\n\\n\\nRays\\n64.17\\n90\\n\\n\\nAngels\\n154.49\\n89\\n\\n\\nTigers\\n132.30\\n88\\n\\n\\nCardinals\\n110.30\\n88\\n\\n\\nDodgers\\n95.14\\n86\\n\\n\\nWhite Sox\\n96.92\\n85\\n\\n\\nBrewers\\n97.65\\n83\\n\\n\\nPhillies\\n174.54\\n81\\n\\n\\nDiamondbacks\\n74.28\\n81\\n\\n\\nPirates\\n63.43\\n79\\n\\n\\nPadres\\n55.24\\n76\\n\\n\\nMariners\\n81.97\\n75\\n\\n\\nMets\\n93.35\\n74\\n\\n\\nBlue Jays\\n75.48\\n73\\n\\n\\nRoyals\\n60.91\\n72\\n\\n\\nMarlins\\n118.07\\n69\\n\\n\\nRed Sox\\n173.18\\n69\\n\\n\\nIndians\\n78.43\\n68\\n\\n\\nTwins\\n94.08\\n66\\n\\n\\nRockies\\n78.06\\n64\\n\\n\\nCubs\\n88.19\\n61\\n\\n\\nAstros\\n60.65\\n55\\n\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b47bfa2f-dd61-4eb8-a47a-6a6e44f40eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <td>Team</td>\n",
      "      <td>\"Payroll (millions)\"</td>\n",
      "      <td>\"Wins\"</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Nationals</td>\n",
      "      <td>81.34</td>\n",
      "      <td>98</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Reds</td>\n",
      "      <td>82.20</td>\n",
      "      <td>97</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Yankees</td>\n",
      "      <td>197.96</td>\n",
      "      <td>95</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Giants</td>\n",
      "      <td>117.62</td>\n",
      "      <td>94</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Braves</td>\n",
      "      <td>83.31</td>\n",
      "      <td>94</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Athletics</td>\n",
      "      <td>55.37</td>\n",
      "      <td>94</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Rangers</td>\n",
      "      <td>120.51</td>\n",
      "      <td>93</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Orioles</td>\n",
      "      <td>81.43</td>\n",
      "      <td>93</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Rays</td>\n",
      "      <td>64.17</td>\n",
      "      <td>90</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Angels</td>\n",
      "      <td>154.49</td>\n",
      "      <td>89</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Tigers</td>\n",
      "      <td>132.30</td>\n",
      "      <td>88</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Cardinals</td>\n",
      "      <td>110.30</td>\n",
      "      <td>88</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Dodgers</td>\n",
      "      <td>95.14</td>\n",
      "      <td>86</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>White Sox</td>\n",
      "      <td>96.92</td>\n",
      "      <td>85</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Brewers</td>\n",
      "      <td>97.65</td>\n",
      "      <td>83</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Phillies</td>\n",
      "      <td>174.54</td>\n",
      "      <td>81</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Diamondbacks</td>\n",
      "      <td>74.28</td>\n",
      "      <td>81</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Pirates</td>\n",
      "      <td>63.43</td>\n",
      "      <td>79</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Padres</td>\n",
      "      <td>55.24</td>\n",
      "      <td>76</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Mariners</td>\n",
      "      <td>81.97</td>\n",
      "      <td>75</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Mets</td>\n",
      "      <td>93.35</td>\n",
      "      <td>74</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Blue Jays</td>\n",
      "      <td>75.48</td>\n",
      "      <td>73</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Royals</td>\n",
      "      <td>60.91</td>\n",
      "      <td>72</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Marlins</td>\n",
      "      <td>118.07</td>\n",
      "      <td>69</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Red Sox</td>\n",
      "      <td>173.18</td>\n",
      "      <td>69</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Indians</td>\n",
      "      <td>78.43</td>\n",
      "      <td>68</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Twins</td>\n",
      "      <td>94.08</td>\n",
      "      <td>66</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Rockies</td>\n",
      "      <td>78.06</td>\n",
      "      <td>64</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Cubs</td>\n",
      "      <td>88.19</td>\n",
      "      <td>61</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Astros</td>\n",
      "      <td>60.65</td>\n",
      "      <td>55</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "print(data[0].metadata[\"text_as_html\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589009d1-81b7-4ad2-ad44-8cabb5653d5a",
   "metadata": {},
   "source": [
    "### Load from URL/Website files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae9fc04-88c7-4b61-800a-dad795a1548a",
   "metadata": {},
   "source": [
    "Usually we use `BeautifulSoup` package to load and parse a HTML or XML file. But it has some limitations.\n",
    "\n",
    "The following code is using `BeautifulSoup` to parse a website. Let's see what limitation it has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "395fe9ad-ee34-454e-be5d-ac0d44a3aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <link href=\"//www.ibm.com/favicon.ico\" rel=\"icon\"/>\n",
      "  <meta content=\"2024-04-12\" name=\"dcterms.date\"/>\n",
      "  <meta content=\"© Copyright IBM Corp. 2016\" name=\"dcterms.rights\"/>\n",
      "  <meta content=\"US\" name=\"geo.country\"/>\n",
      "  <meta content=\"index,follow\" name=\"robots\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <meta content=\"The page you requested cannot be displayed (HTTP response code 403)\" name=\"description\"/>\n",
      "  <meta content=\"The page you requested cannot be displayed (HTTP response code 403)\" name=\"keywords\"/>\n",
      "  <title>\n",
      "   IBM notice: The page you requested cannot be displayed\n",
      "  </title>\n",
      "  <script>\n",
      "   window.digitalData = {\n",
      "\t\t\tpage: {\n",
      "\t\t\t\tcategory: {\n",
      "\t\t\t\t\tprimaryCategory: 'error'\n",
      "\t\t\t\t},\n",
      "\t\t\t\tpageInfo: {\n",
      "\t\t\t\t\tlanguage: 'en-ZZ',\n",
      "\t\t\t\t\tibm: {\n",
      "\t\t\t\t\t\tcountry: 'ZZ',\n",
      "\t\t\t\t\t\tsiteID: 'error',\n",
      "\t\t\t\t\t\tsubject: 'ZZ999',\n",
      "\t\t\t\t\t\ttype: 'CT030'\n",
      "\t\t\t\t\t},\n",
      "\t\t\t\t},\n",
      "\t\t\t\tisDataLayerReady: true,\n",
      "\t\t\t},\n",
      "\t\t};\n",
      "  </script>\n",
      "  <!-- Styles -->\n",
      "  <link href=\"https://1.www.s81c.com/common/carbon-for-ibm-dotcom/tag/latest/plex.css\" rel=\"stylesheet\">\n",
      "   <link href=\"https://1.www.s81c.com/common/carbon/web-components/tag/latest/themes.css\" rel=\"stylesheet\"/>\n",
      "   <link href=\"https://1.www.s81c.com/common/carbon/web-components/tag/latest/cssgrid.css\" rel=\"stylesheet\"/>\n",
      "   <link href=\"https://1.www.s81c.com/common/carbon/plex/sans.css\" rel=\"stylesheet\"/>\n",
      "   <!-- Styles customization -->\n",
      "   <style>\n",
      "    .content-wrapper {position: relative;}\n",
      "\t\t.content-wrapper.lead {padding: 0 5vw 60px;background: #f4f4f4;}\n",
      "\t\t.content-wrapper.side {padding: 50px 5vw;background: #f4f4f4;}\n",
      "\t\t.content-wrapper.lead c4d-content-block-simple {padding-bottom: 0; max-width: 600px; margin-top:0; margin-bottom:0; padding-top: 4rem;}\n",
      "\t\t.content-wrapper.lead .search-wrapper {max-width: 350px;margin: 1rem;}\n",
      "\t\t.search-wrapper > c4d-search-with-typeahead {z-index: 10;}\n",
      "\t\tc4d-link-with-icon {\n",
      "\t\t\t--cds-link-primary: #0f62fe;\n",
      "\t\t\t--cds-link-visited: #0f62fe;\n",
      "\t\t}\n",
      "\t\t.error-code {position: absolute;bottom: -3.5vw;right: 0;font-size: 30vw;font-weight: 700;line-height: 30vw;color: #000;opacity: .05;}\n",
      "   </style>\n",
      "   <!-- Import Components -->\n",
      "   <script src=\"https://1.www.s81c.com/common/carbon-for-ibm-dotcom/tag/latest/masthead.min.js\" type=\"module\">\n",
      "   </script>\n",
      "   <script src=\"https://1.www.s81c.com/common/carbon-for-ibm-dotcom/tag/latest/footer.min.js\" type=\"module\">\n",
      "   </script>\n",
      "   <script src=\"https://1.www.s81c.com/common/carbon-for-ibm-dotcom/tag/latest/content-block-simple.min.js\" type=\"module\">\n",
      "   </script>\n",
      "   <script src=\"https://1.www.s81c.com/common/carbon-for-ibm-dotcom/tag/latest/link-with-icon.min.js\" type=\"module\">\n",
      "   </script>\n",
      "   <script src=\"https://1.www.s81c.com/common/carbon-for-ibm-dotcom/tag/latest/cta-block.min.js\" type=\"module\">\n",
      "   </script>\n",
      "   <script src=\"https://1.www.s81c.com/common/carbon-for-ibm-dotcom/tag/latest/link-list.min.js\" type=\"module\">\n",
      "   </script>\n",
      "   <script src=\"https://1.www.s81c.com/common/carbon-for-ibm-dotcom/tag/latest/button.min.js\" type=\"module\">\n",
      "   </script>\n",
      "   <!-- IBM Tag Management and Site Analytics -->\n",
      "   <script async=\"async\" src=\"//1.www.s81c.com/common/stats/ibm-common.js\" type=\"text/javascript\">\n",
      "   </script>\n",
      "  </link>\n",
      " </head>\n",
      " <body>\n",
      "  <!-- MASTHEAD -->\n",
      "  <header>\n",
      "   <c4d-masthead-container data-endpoint=\"/common/carbon-for-ibm-dotcom/translations/masthead-footer/v2.1\">\n",
      "   </c4d-masthead-container>\n",
      "  </header>\n",
      "  <!-- /MASTHEAD -->\n",
      "  <!-- BODY -->\n",
      "  <section class=\"content-wrapper lead cds-theme-zone-g10\">\n",
      "   <div class=\"error-code\">\n",
      "    403\n",
      "   </div>\n",
      "   <c4d-content-block-simple>\n",
      "    <c4d-content-block-heading size=\"sm\" style=\"font-size: 2.75rem; color: #e71d32;\">\n",
      "     Oops — that's not right!\n",
      "    </c4d-content-block-heading>\n",
      "    <c4d-content-block-copy>\n",
      "     <c4d-content-block-paragraph size=\"md\" style=\"font-size: 1.25rem;\">\n",
      "      The page you requested cannot be displayed.\n",
      "     </c4d-content-block-paragraph>\n",
      "    </c4d-content-block-copy>\n",
      "    <c4d-content-block-copy style=\"margin-top:-2rem;\">\n",
      "     <c4d-content-block-paragraph size=\"md\" style=\"font-size: 1.25rem; font-weight: 700;\">\n",
      "      403: Forbidden\n",
      "     </c4d-content-block-paragraph>\n",
      "    </c4d-content-block-copy>\n",
      "    <c4d-content-block-copy>\n",
      "     <c4d-content-block-paragraph size=\"sm\" style=\"font-size: 1rem;\">\n",
      "      <p>\n",
      "       Incident Number: 18.dc3e1602.1763920733.c04dbe8\n",
      "      </p>\n",
      "     </c4d-content-block-paragraph>\n",
      "    </c4d-content-block-copy>\n",
      "    <c4d-content-block-copy size=\"md\">\n",
      "     Suggested actions\n",
      "    </c4d-content-block-copy>\n",
      "    <c4d-content-block-copy size=\"sm\">\n",
      "     <cds-unordered-list>\n",
      "      <cds-list-item>\n",
      "       If you typed the address, please make sure that the spelling is correct.\n",
      "\t\t\t\t\tNote: Most addresses are case sensitive.\n",
      "      </cds-list-item>\n",
      "      <cds-list-item>\n",
      "       For information on IBM offerings, start from the\n",
      "       <a href=\"http://www.ibm.com/\">\n",
      "        IBM homepage\n",
      "       </a>\n",
      "       .\n",
      "      </cds-list-item>\n",
      "      <cds-list-item>\n",
      "       <a href=\"https://www.ibm.com/search\">\n",
      "        Search the IBM Web site\n",
      "       </a>\n",
      "       .\n",
      "      </cds-list-item>\n",
      "     </cds-unordered-list>\n",
      "    </c4d-content-block-copy>\n",
      "    <div style=\"margin: 3rem 1rem;\">\n",
      "     <c4d-content-block-copy size=\"md\">\n",
      "      Get assistance\n",
      "     </c4d-content-block-copy>\n",
      "     <c4d-content-block-copy size=\"sm\">\n",
      "      This option lets you send an information request and tell us about a broken link. You will receive an e-mail from us to help you find what you need.\n",
      "     </c4d-content-block-copy>\n",
      "     <c4d-video-cta-container>\n",
      "      <c4d-button cta-type=\"local\" data-autoid=\"c4d--button\" download=\"\" has-main-content=\"\" href=\"https://www.ibm.com/contact\" isexpressive=\"\" kind=\"tertiary\" size=\"lg\" tooltip-alignment=\"\" tooltip-position=\"top\" type=\"button\">\n",
      "       Report the issue\n",
      "      </c4d-button>\n",
      "     </c4d-video-cta-container>\n",
      "    </div>\n",
      "   </c4d-content-block-simple>\n",
      "  </section>\n",
      "  <!-- /BODY -->\n",
      "  <!-- FOOTER -->\n",
      "  <footer>\n",
      "   <c4d-footer-container>\n",
      "   </c4d-footer-container>\n",
      "  </footer>\n",
      "  <!-- /FOOTER -->\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.ibm.com/topics/langchain'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee19e5-66cd-4e36-ab0f-fdf073e6dcc3",
   "metadata": {},
   "source": [
    "From the print output, we can see that `BeautifulSoup` not only load the web content, but also a lot of HTML tags and external links, which are not necessary if we just want to load the text content of the web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1855c-7775-4e7a-928e-78ceb27b413d",
   "metadata": {},
   "source": [
    "So LangChain's `WebBaseLoader` can effectively address this limitation.\n",
    "\n",
    "`WebBaseLoader` is designed to extract all text from HTML webpages and convert it into a document format suitable for further processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620586a-d773-4b7a-86fd-4a7042ce0aa8",
   "metadata": {},
   "source": [
    "#### Load from single web page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f005d78-28bd-4400-b0cb-2e4215ab18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://www.ibm.com/topics/langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbe4d3ec-0fb0-4232-ab0e-11019fab7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35d517eb-e473-4b34-9a18-61adc8356bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.ibm.com/topics/langchain', 'title': 'IBM notice: The page you requested cannot be displayed', 'description': 'The page you requested cannot be displayed (HTTP response code 403)', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIBM notice: The page you requested cannot be displayed\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t403\\n\\t\\t\\n\\nOops — that's not right!\\n\\nThe page you requested cannot be displayed.\\n\\n\\n403: Forbidden\\n\\n\\nIncident Number: 18.e3b31402.1763920748.3a98a54b\\n\\nSuggested actions\\n\\n\\nIf you typed the address, please make sure that the spelling is correct.\\n\\t\\t\\t\\t\\tNote: Most addresses are case sensitive.\\nFor information on IBM offerings, start from the IBM homepage.\\nSearch the IBM Web site.\\n\\n\\n\\nGet assistance\\nThis option lets you send an information request and tell us about a broken link. You will receive an e-mail from us to help you find what you need.\\n\\n\\n\\t\\t\\t\\t\\t  Report the issue\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8631591b-ca91-407f-8800-4860c56895a4",
   "metadata": {},
   "source": [
    "#### Load from multiple web pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e340a3c-5697-4b31-9d13-ec4cd4ecf422",
   "metadata": {},
   "source": [
    "You can load multiple webpages simultaneously by passing a list of URLs to the loader. This will return a list of documents corresponding to the order of the URLs provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5d53a11-ef9d-4d8b-94fa-c3e7ce5013c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.ibm.com/topics/langchain', 'title': 'IBM notice: The page you requested cannot be displayed', 'description': 'The page you requested cannot be displayed (HTTP response code 403)', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIBM notice: The page you requested cannot be displayed\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t403\\n\\t\\t\\n\\nOops — that's not right!\\n\\nThe page you requested cannot be displayed.\\n\\n\\n403: Forbidden\\n\\n\\nIncident Number: 18.e3b31402.1763920757.3a9a155f\\n\\nSuggested actions\\n\\n\\nIf you typed the address, please make sure that the spelling is correct.\\n\\t\\t\\t\\t\\tNote: Most addresses are case sensitive.\\nFor information on IBM offerings, start from the IBM homepage.\\nSearch the IBM Web site.\\n\\n\\n\\nGet assistance\\nThis option lets you send an information request and tell us about a broken link. You will receive an e-mail from us to help you find what you need.\\n\\n\\n\\t\\t\\t\\t\\t  Report the issue\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'https://www.redhat.com/en/topics/ai/what-is-instructlab', 'title': 'What is InstructLab?', 'description': 'InstructLab simplifies the process of customizing large language models with private data.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\nWhat is InstructLab?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to contentRed HatNavigation\\n              AI\\n          Our approach\\n                                      News and insights\\n                                  \\n                                      Technical blog\\n                                  \\n                                      Research\\n                                  \\n                                      Live AI events\\n                                  Explore AI at Red HatOur portfolio\\n                                      Red Hat AI\\n                                  \\n                                      Red Hat Enterprise Linux AI\\n                                  \\n                                      Red Hat OpenShift AI\\n                                  \\n                                      Red Hat AI Inference Server\\n                                  Engage & learn\\n                                      AI learning hub\\n                                  \\n                                      AI partners\\n                                  \\n                                      Services for AI\\n                                  \\n              Hybrid cloud\\n          Platform solutionsArtificial intelligenceBuild, deploy, and monitor AI models and apps.Linux standardizationGet consistency across operating environments.Application developmentSimplify the way you build, deploy, and manage apps.AutomationScale automation and unite tech, teams, and environments.Use casesVirtualizationModernize operations for virtualized and containerized workloads.Digital sovereignty Control and protect critical infrastructure. SecurityCode, build, deploy, and monitor security-focused software.Edge computingDeploy workloads closer to the source with edge technology.Explore solutionsSolutions by industry\\n                                      Automotive\\n                                  \\n                                      Financial services\\n                                  \\n                                      Healthcare\\n                                  \\n                                      Industrial sector\\n                                  \\n                                      Media and entertainment\\n                                  \\n                                      Public sector (Global)\\n                                  \\n                                      Public sector (U.S.)\\n                                  \\n                                      Telecommunications\\n                                  Discover cloud technologiesLearn how to use our cloud products and solutions at your own pace in the Red\\xa0Hat® Hybrid Cloud Console. \\n              Products\\n          PlatformsRed Hat AIDevelop and deploy AI solutions across the hybrid cloud.New versionRed Hat Enterprise LinuxSupport hybrid cloud innovation on a flexible operating system.Red Hat OpenShiftBuild, modernize, and deploy apps at scale.Red Hat Ansible Automation PlatformImplement enterprise-wide automation.Featured\\n                                      Red Hat OpenShift Virtualization Engine\\n                                  \\n                                      Red Hat OpenShift Service on AWS\\n                                  \\n                                      Microsoft Azure Red Hat OpenShift\\n                                  See all productsTry & buy\\n                                      Start a trial\\n                                  \\n                                      Buy online\\n                                  \\n                                      Integrate with major cloud providers\\n                                  Services & support\\n                                      Consulting\\n                                  \\n                                      Product support\\n                                  \\n                                      Services for AI\\n                                  \\n                                      Technical Account Management\\n                                  Explore services\\n              Training\\n          Training & certification\\n                                      Courses and exams\\n                                  \\n                                      Certifications\\n                                  \\n                                      Red Hat Academy\\n                                  \\n                                      Learning community\\n                                  \\n                                      Learning subscription\\n                                  Explore trainingFeatured\\n                                      Red Hat Certified System Administrator exam\\n                                  \\n                                      Red Hat System Administration I\\n                                  \\n                                      Red Hat Learning Subscription trial (No cost)\\n                                  \\n                                      Red Hat Certified Engineer exam\\n                                  \\n                                      Red Hat Certified OpenShift Administrator exam\\n                                  Services\\n                                      Consulting\\n                                  \\n                                      Partner training\\n                                  \\n                                      Product support\\n                                  \\n                                      Services for AI\\n                                  \\n                                      Technical Account Management\\n                                  \\n              Learn\\n          Build your skills\\n                                      Documentation\\n                                  \\n                                      Hands-on labs\\n                                  \\n                                      Hybrid cloud learning hub\\n                                  \\n                                      Interactive learning experiences\\n                                  \\n                                      Training and certification\\n                                  More ways to learn\\n                                      Blog\\n                                  \\n                                      Events and webinars\\n                                  \\n                                      Podcasts and video series\\n                                  \\n                                      Red Hat TV\\n                                  \\n                                      Resource library\\n                                  For developersDiscover resources and tools to help you build, deliver, and manage cloud-native applications and services.\\n              Partners\\n          For customers\\n                                      Our partners\\n                                  \\n                                      Red Hat Ecosystem Catalog\\n                                  Find a partnerFor partners\\n                                      Partner Connect\\n                                  \\n                                      Become a partner\\n                                  \\n                                      Training\\n                                  \\n                                      Support\\n                                  Access the partner portalBuild solutions powered by trusted partnersFind solutions from our collaborative community of experts and technologies in the Red\\xa0Hat® Ecosystem Catalog.\\n            Search\\n        ×I\\'d like to:Start a trialManage subscriptionsSee Red Hat jobsExplore tech topicsContact salesContact customer serviceHelp me find:DocumentationDeveloper resourcesSkills assessmentsArchitecture centerSecurity updatesSupport casesI want to learn more about:AIApplication modernizationAutomationCloud-native applicationsLinuxVirtualizationConsoleDocsSupportNew For you\\n            RecommendedWe\\'ll recommend resources you may like as you browse. Try these suggestions for now.Product trial centerCourses and examsAll productsTech topicsResource libraryLog inGet more with a Red Hat account Console access Event registration Training & trials World-class supportA subscription may be required for some services.Log in or registerChange page language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañolContact us\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                                Topics                                                            Open source                            \\n                    What is InstructLab?                     \\n\\n\\nWhat is InstructLab?\\n\\n\\n\\n\\nPublished  October 6, 2025•4-minute readCopy URL\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to sectionWhat is InstructLab?How does InstructLab work?How is InstructLab different?What is Red Hat InstructLab on IBM Cloud? \\n\\n\\n\\nWhat is InstructLab?InstructLab simplifies the process of customizing\\xa0large language models (LLMs) with private data.\\xa0LLMs serve as the foundation for\\xa0generative AI (gen AI), like chatbots and coding assistants. These LLMs can be proprietary (such as OpenAI’s GPT models and Anthropic’s Claude models) or offer varying degrees of openness around pretraining data and usage restrictions (such as Meta’s Llama models, Mistral AI’s Mistral models, and\\xa0IBM’s Granite models).AI practitioners often need to adapt a pretrained LLM to suit a particular business purpose. But there are limits to the ways you can modify an LLM:Fine-tuning an LLM to understand a specific area of knowledge or skills typically involves expensive, resource-intensive training.There’s no way to incorporate improvements back to the LLM, and thus no way for models to continuously improve from user contributions.LLM refinements have typically required large amounts of human-generated data, which can be time-consuming and expensive to get.InstructLab follows an approach that punches through those limitations. It can enhance an LLM using far less human-generated information and far fewer computing resources than are typically used to retrain a model.InstructLab is named after and based on IBM Research’s work on Large-scale Alignment for chatBots, abbreviated as LAB. The LAB method is described in a\\xa02024 research paper by members of the MIT-IBM Watson AI Lab and IBM Research.InstructLab is not model-specific. It can provide supplemental skills and knowledge fine-tuning to an LLM of your choice. This “tree of skills and knowledge” improves continuously from contributions and can be applied to support regular builds of an enhanced LLM.\\xa0The InstructLab project prioritizes fast iteration and intends to retrain models on a regular basis. Organizations can also use the InstructLab model alignment tools to train their own private LLMs with their own proprietary skills and knowledge.How to apply AI at the enterprise\\xa0 \\nHow does InstructLab work?The LAB method consists of 3 components:Taxonomy-driven data curation. Taxonomy is a set of diverse training data curated by humans as examples of new knowledge and skills for the model.Large-scale synthetic data generation. The model is then used to generate new examples based on the seed training data. Recognizing that synthetic data can vary in quality, the LAB method adds an automated step to refine the example answers, making sure they’re grounded and safe.Iterative, large-scale alignment tuning. Finally, the model is retrained based on the set of synthetic data. The LAB method includes 2 tuning phases: knowledge tuning, followed by skill tuning.The dataset grows through contributions of skills and knowledge, with each addition improving the quality of the model being tuned.\\xa0 \\n\\nRed Hat resourcesKeep reading\\n\\n\\n\\n\\nHow is InstructLab different from other methods of training an LLM?Let’s compare InstructLab to the other ways of customizing an LLM to serve your domain-specific use cases.PretrainingPretraining represents the deepest level of model customization, and allows organizations to curate what training data shapes the models’ foundational understanding.\\xa0During pretraining, an LLM is trained to predict the next token using trillions of tokens of unlabeled data. This gets really expensive, sometimes requiring thousands of GPUs and months of time. Pretraining a highly capable LLM is only possible for organizations with significant resources.Alignment tuningAfter pretraining, LLMs undergo alignment tuning to make the model’s answers as accurate and useful as possible. The 1st step in alignment tuning is typically\\xa0instruction tuning, in which a model is trained directly on specific tasks of interest.\\xa0Next is\\xa0preference tuning, which can include reinforcement learning from human feedback (RLHF). In this step, humans test the model and rate its output, noting if the model’s answers are preferred or unpreferred. An RLHF process may include multiple rounds of feedback and refinement to optimize a model.Researchers have found that the amount of feedback at this alignment tuning stage can be much smaller than the initial set of training data―tens of thousands of human annotations, compared to the trillions of tokens of data required for pretraining―and still unlock latent capabilities of the model.InstructLabThe LAB method emerged from the idea that it should be possible to realize the benefits of model alignment from an even smaller set of human-generated data. An AI model can use a handful of human examples to generate a large amount of synthetic data―then refine that list for quality―and use that high-quality synthetic data set for further tuning and training. In contrast to instruction tuning, which typically needs thousands of examples of human feedback, LAB can make a model significantly better using relatively few examples provided by humans.How is InstructLab different from retrieval-augmented generation (RAG)?The short answer is InstructLab and retrieval-augmented generation (RAG) solve different problems.RAG is a cost-efficient method for supplementing an LLM with domain-specific knowledge that wasn’t part of its pretraining. RAG makes it possible for a chatbot to accurately answer questions related to a specific field or business without retraining the model.\\xa0With RAG, knowledge documents are stored in a vector database, then retrieved in chunks and sent to the model as part of user queries. This is helpful for anyone who wants to add proprietary data to an LLM without giving up control of their information, or who needs an LLM to access timely information.\\xa0This is in contrast to the InstructLab method, which sources end-user contributions to support regular builds of an enhanced version of an LLM. InstructLab helps add knowledge and unlock new skills of an LLM.It’s possible to \"supercharge\" a RAG process by using the RAG technique on an InstructLab-tuned model.Learn more about RAG \\nWhat is Red Hat InstructLab on IBM Cloud?Red Hat AI InstructLab on IBM Cloud allows for contributions to an LLM without the need to own and operate hardware infrastructure.On its own, Red Hat InstructLab is an open source project that simplifies LLM development, enabling a cost-effective approach to align models with less data and fewer resources.IBM Cloud is an enterprise cloud platform designed for even the most regulated industries, delivering a highly resilient, performant, secure, and compliant cloud.Together, Red Hat AI InstructLab on IBM Cloud offers a scalable, cost-effective solution to simplify, scale, and secure the alignment of your LLMs to your organization’s unique use-cases.Learn more about Red Hat InstructLab on IBM Cloud \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHub\\n \\n\\n\\n\\n\\n\\n\\nThe official Red\\xa0Hat blog\\n\\n\\n\\nGet the latest information about our ecosystem of customers, partners, and communities.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKeep reading \\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Red\\xa0Hat product trialsOur no-cost product trials help you gain hands-on experience, prepare for a certification, or assess if a product is right for your organization.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKeep reading \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKeep reading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Istio?\\n\\n\\n\\n        Find out more about Istio, an open source service mesh that controls how microservices share data with one another.\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the article \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is CentOS Stream?\\n\\n\\n\\n        CentOS Stream is a Linux® development platform where open source community members can contribute to Red\\xa0Hat® Enterprise Linux in tandem with Red\\xa0Hat developers. \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the article \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is KVM?\\n\\n\\n\\n        Kernel-based virtual machines (KVM) are an open source virtualization technology that turns Linux into a hypervisor.\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the article \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen source resources\\n\\n\\n\\n\\n\\n\\nFeatured product\\n\\n\\n\\n\\n\\n\\n\\nRed Hat AI\\n\\nFlexible solutions that accelerate AI solution development and deployment across hybrid cloud environments.\\n\\n\\n\\n\\n\\n\\n\\nSee all products \\n\\n\\n\\n\\n\\n\\n\\nRelated content\\n\\n\\n\\n\\n\\n\\nBlog post\\n\\nForging the open path: How Red Hat engineering is adopting AI and what it means for open source\\n\\n\\n\\nE-book\\n\\nMaximize AI innovation with open source models\\n\\n\\n\\nE-book\\n\\nTop considerations for building a production-ready AI/ML environment\\n\\n\\n\\nE-book\\n\\nThe adaptable enterprise: Why AI readiness is disruption readiness\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRelated articles\\n\\n\\n\\n\\n\\n\\nWhat is llm-d?\\n\\n\\nWhat is distributed inference?\\n\\n\\nWhat is Model Context Protocol (MCP)?\\n\\n\\nAIOps explained\\n\\n\\nWhat is AI security? \\n\\n\\nAgentic AI vs. generative AI\\n\\n\\nWhat are large language models?\\n\\n\\nWhat is Models-as-a-Service?\\n\\n\\nWhat is AI in the public sector?\\n\\n\\nSLMs vs LLMs: What are small language models?\\n\\n\\nWhat is enterprise AI?\\n\\n\\nWhat is Istio?\\n\\n\\nWhat is parameter-efficient fine-tuning (PEFT)?\\n\\n\\nWhy choose Red Hat Ansible Automation Platform as your AI foundation?\\n\\n\\nLoRA vs. QLoRA\\n\\n\\nWhat is CentOS Stream?\\n\\n\\nWhat is vLLM?\\n\\n\\nWhat is AI inference?\\n\\n\\nPredictive AI vs generative AI\\n\\n\\nWhat is agentic AI?\\n\\n\\nWhat is KVM?\\n\\n\\nWhat are Granite models? \\n\\n\\nRAG vs. fine-tuning\\n\\n\\nWhat is Podman Desktop?\\n\\n\\nUnderstanding AI in telecommunications with Red Hat\\n\\n\\nEdge solutions for real-time decision making\\n\\n\\nWhat are CentOS replacements?\\n\\n\\nWhat is CentOS?\\n\\n\\nWhat are intelligent applications?\\n\\n\\nWhat is Podman?\\n\\n\\nWhat is retrieval-augmented generation?\\n\\n\\nWhat is Helm?\\n\\n\\nWhat is Argo CD?\\n\\n\\nWhat is an AI platform?\\n\\n\\nWhat is LLMops\\n\\n\\nWhat is deep learning?\\n\\n\\nWhat are predictive analytics\\n\\n\\nAI in banking\\n\\n\\nWhat is MicroShift?\\n\\n\\nAI infrastructure explained\\n\\n\\nUnderstanding AI/ML use cases\\n\\n\\nWhat is MLOps?\\n\\n\\nWhat are foundation models for AI?\\n\\n\\nHow Kubernetes can help AI/ML\\n\\n\\nWhat is generative AI?\\n\\n\\nWhat is edge AI?\\n\\n\\nOpenJDK versus Oracle JDK\\n\\n\\nWhat is Cloud Foundry?\\n\\n\\nWhat is Kubeflow?\\n\\n\\nWhat is Buildah?\\n\\n\\nAccelerate MLOps with Red Hat OpenShift\\n\\n\\nUnderstanding Ansible, Terraform, Puppet, Chef, and Salt\\n\\n\\nAnsible vs. Chef: What you need to know\\n\\n\\nAnsible vs. Salt: What you need to know\\n\\n\\nWhat is Linux?\\n\\n\\nWhat\\'s the best Linux distro for you?\\n\\n\\nWhat is machine learning?\\n\\n\\nAnsible vs. Puppet: What you need to know\\n\\n\\nRed Hat OpenShift vs. OKD\\n\\n\\nWhat is AI in healthcare? \\n\\n\\nWhat is Apache Kafka?\\n\\n\\nSpring on Kubernetes with Red Hat OpenShift\\n\\n\\nWhy run Apache Kafka on Kubernetes?\\n\\n\\nAnsible vs. Terraform, clarified\\n\\n\\nAnsible vs. Red Hat Ansible Automation Platform\\n\\n\\nWhat is Skopeo?\\n\\n\\nUsing Helm with Red Hat OpenShift\\n\\n\\nWhat is Grafana?\\n\\n\\nWhat is open source software?\\n\\n\\nOpen source vs. proprietary software in vehicles\\n\\n\\nWhat is KubeLinter?\\n\\n\\nWhat is RKT?\\n\\n\\nWhat is Kogito?\\n\\n\\nWhat was CoreOS and CoreOS container Linux\\n\\n\\nWhat is Jaeger?\\n\\n\\nWhat is open source?\\n\\n\\nWhat is a data lake?\\n\\n\\nWhat is Clair?\\n\\n\\nWhat is Knative?\\n\\n\\nWhat is etcd?\\n\\n\\nWhat is Docker?\\n\\n\\n\\n\\n\\n\\n\\nMore about this topic \\n\\n\\n\\n\\n\\n\\n\\n\\nLinkedInYouTubeFacebookXPlatformsRed Hat AIRed Hat Enterprise LinuxRed Hat OpenShiftRed Hat Ansible Automation PlatformSee all productsToolsTraining and certificationMy accountCustomer supportDeveloper resourcesFind a partnerRed Hat Ecosystem CatalogDocumentationTry, buy, & sellProduct trial centerRed Hat StoreBuy online (Japan)ConsoleCommunicateContact salesContact customer serviceContact trainingSocial\\n    About Red Hat\\n\\n    Red Hat is an open hybrid cloud technology leader, delivering a consistent, comprehensive foundation for transformative IT and artificial intelligence (AI) applications in the enterprise. As a trusted adviser to the Fortune 500, Red Hat offers cloud, developer, Linux, automation, and application platform technologies, as well as award-winning services.\\nOur companyHow we workCustomer success storiesAnalyst relationsNewsroomOpen source commitmentsOur social impactJobsChange page language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañolRed Hat legal and privacy linksAbout Red HatJobsEventsLocationsContact Red HatRed Hat BlogInclusion at Red HatCool Stuff StoreRed Hat Summit© 2025 Red HatRed Hat legal and privacy linksPrivacy statementTerms of useAll policies and guidelinesDigital accessibility\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader([\"https://www.ibm.com/topics/langchain\", \"https://www.redhat.com/en/topics/ai/what-is-instructlab\"])\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7ea39-3bc2-43ba-87d5-241c37c79e46",
   "metadata": {},
   "source": [
    "### Load from WORD files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d0283-b834-4a87-a7c9-b9f8749d3f66",
   "metadata": {},
   "source": [
    "`Docx2txtLoader` is utilized to convert Word documents into a document format suitable for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c8250b5-36c5-448a-98be-e6f0cf35c2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-23 21:29:24--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/94hiHUNLZdb0bLMkrCh79g/file-sample.docx\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 198.23.119.245\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|198.23.119.245|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1311881 (1.3M) [application/vnd.openxmlformats-officedocument.wordprocessingml.document]\n",
      "Saving to: ‘file-sample.docx’\n",
      "\n",
      "file-sample.docx    100%[===================>]   1.25M   848KB/s    in 1.5s    \n",
      "\n",
      "2025-11-23 21:29:27 (848 KB/s) - ‘file-sample.docx’ saved [1311881/1311881]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/94hiHUNLZdb0bLMkrCh79g/file-sample.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "716de707-5fc4-4412-9a3a-2595991e69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Docx2txtLoader(\"file-sample.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7e7f747-37d3-4abf-970c-369caec23918",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca01b8c6-b30d-4133-9c58-c0df8023a08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'file-sample.docx'}, page_content='Demonstration of DOCX support in calibre\\n\\nThis document demonstrates the ability of the calibre DOCX Input plugin to convert the various typographic features in a Microsoft Word (2007 and newer) document. Convert this document to a modern ebook format, such as AZW3 for Kindles or EPUB for other ebook readers, to see it in action.\\n\\nThere is support for images, tables, lists, footnotes, endnotes, links, dropcaps and various types of text and paragraph level formatting.\\n\\nTo see the DOCX conversion in action, simply add this file to calibre using the “Add Books” button and then click “Convert”.  Set the output format in the top right corner of the conversion dialog to EPUB or AZW3 and click “OK”.\\n\\n\\n\\nText Formatting\\n\\nInline formatting\\n\\nHere, we demonstrate various types of inline text formatting and the use of embedded fonts.\\n\\nHere is some bold, italic, bold-italic, underlined and struck out  text. Then, we have a superscript and a subscript. Now we see some red, green and blue text. Some text with a yellow highlight. Some text in a box. Some text in inverse video.\\n\\nA paragraph with styled text: subtle emphasis  followed by strong text and intense emphasis. This paragraph uses document wide styles for styling rather than inline text properties as demonstrated in the previous paragraph — calibre can handle both with equal ease.\\n\\nFun with fonts\\n\\nThis document has embedded the Ubuntu font family. The body text is in the Ubuntu typeface, here is some text in the Ubuntu Mono typeface, notice how every letter has the same width, even i and m. Every embedded font will automatically be embedded in the output ebook during conversion. \\n\\nParagraph level formatting\\n\\nYou can do crazy things with paragraphs, if the urge strikes you. For instance this paragraph is right aligned and has a right border. It has also been given a light gray background.\\n\\nFor the lovers of poetry amongst you, paragraphs with hanging indents, like this often come in handy. You can use hanging indents to ensure that a line of poetry retains its individual identity as a line even when the screen is  too narrow to display it as a single line. Not only does this paragraph have a hanging indent, it is also has an extra top margin, setting it apart from the preceding paragraph.\\n\\nTables\\n\\nITEM\\n\\nNEEDED\\n\\nBooks\\n\\n1\\n\\nPens\\n\\n3\\n\\nPencils\\n\\n2\\n\\nHighlighter\\n\\n2 colors\\n\\nScissors\\n\\n1 pair\\n\\nTables in Word can vary from the extremely simple to the extremely complex. calibre tries to do its best when converting tables. While you may run into trouble with the occasional table, the vast majority of common cases should be converted very well, as demonstrated in this section. Note that for optimum results, when creating tables in Word, you should set their widths using percentages, rather than absolute units.  To the left of this paragraph is a floating two column table with a nice green border and header row.\\n\\nNow let’s look at a fancier table—one with alternating row colors and partial borders. This table is stretched out to take 100% of the available width.\\n\\nCity or Town\\n\\nPoint A\\n\\nPoint B\\n\\nPoint C\\n\\nPoint D\\n\\nPoint E\\n\\nPoint A\\n\\n—\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPoint B\\n\\n87\\n\\n—\\n\\n\\n\\n\\n\\n\\n\\nPoint C\\n\\n64\\n\\n56\\n\\n—\\n\\n\\n\\n\\n\\nPoint D\\n\\n37\\n\\n32\\n\\n91\\n\\n—\\n\\n\\n\\nPoint E\\n\\n93\\n\\n35\\n\\n54\\n\\n43\\n\\n—\\n\\n\\n\\nNext, we see a table with special formatting in various locations. Notice how the formatting for the header row and sub header rows is preserved.\\n\\nCollege\\n\\nNew students\\n\\nGraduating students\\n\\nChange\\n\\n\\n\\nUndergraduate\\n\\n\\n\\n\\n\\nCedar University\\n\\n110\\n\\n103\\n\\n+7\\n\\nOak Institute\\n\\n202\\n\\n210\\n\\n-8\\n\\n\\n\\nGraduate\\n\\n\\n\\n\\n\\nCedar University\\n\\n24\\n\\n20\\n\\n+4\\n\\nElm College\\n\\n43\\n\\n53\\n\\n-10\\n\\nTotal\\n\\n998\\n\\n908\\n\\n90\\n\\nSource: Fictitious data, for illustration purposes only\\n\\nNext, we have something a little more complex, a nested table, i.e. a table inside another table. Additionally, the inner table has some of its cells merged. The table is displayed horizontally centered.\\n\\nOne\\n\\nThree\\n\\nTwo\\n\\n\\n\\nFour\\n\\n\\n\\nTo the left is a table inside a table, with some cells merged.\\n\\n\\n\\nWe end with a fancy calendar, note how much of the original formatting is preserved. Note that this table will only display correctly on relatively wide screens. In general, very wide tables or tables whose cells have fixed width requirements don’t fare well in ebooks.\\n\\nDecember 2007\\n\\nSun\\n\\n\\n\\nMon\\n\\n\\n\\nTue\\n\\n\\n\\nWed\\n\\n\\n\\nThu\\n\\n\\n\\nFri\\n\\n\\n\\nSat\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\n\\n\\n\\n3\\n\\n\\n\\n4\\n\\n\\n\\n5\\n\\n\\n\\n6\\n\\n\\n\\n7\\n\\n\\n\\n8\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n9\\n\\n\\n\\n10\\n\\n\\n\\n11\\n\\n\\n\\n12\\n\\n\\n\\n13\\n\\n\\n\\n14\\n\\n\\n\\n15\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\n\\n17\\n\\n\\n\\n18\\n\\n\\n\\n19\\n\\n\\n\\n20\\n\\n\\n\\n21\\n\\n\\n\\n22\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n23\\n\\n\\n\\n24\\n\\n\\n\\n25\\n\\n\\n\\n26\\n\\n\\n\\n27\\n\\n\\n\\n28\\n\\n\\n\\n29\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n30\\n\\n\\n\\n31\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStructural Elements\\n\\nMiscellaneous structural elements you can add to your document, like footnotes, endnotes, dropcaps and the like. \\n\\nFootnotes & Endnotes\\n\\nFootnotes and endnotes are automatically recognized and both are converted to endnotes, with backlinks for maximum ease of use in ebook devices.\\n\\nDropcaps\\n\\nD\\n\\nrop caps are used to emphasize the leading paragraph at the start of a section. In Word it is possible to specify how many lines of text a drop-cap should use. Because of limitations in ebook technology, this is not possible when converting.  Instead, the converted drop cap will use font size and line height to simulate the effect as well as possible. While not as good as the original, the result is usually tolerable. This paragraph has a “D” dropcap set to occupy three lines of text with a font size of 58.5 pts. Depending on the screen width and capabilities of the device you view the book on, this dropcap can look anything from perfect to ugly.\\n\\nLinks\\n\\nTwo kinds of links are possible, those that refer to an external website and those that refer to locations inside the document itself. Both are supported by calibre. For example, here is a link pointing to the calibre download page. Then we have a link that points back to the section on paragraph level formatting in this document.\\n\\nTable of Contents\\n\\nThere are two approaches that calibre takes when generating a Table of Contents. The first is if the Word document has a Table of Contents itself. Provided that the Table of Contents uses hyperlinks, calibre will automatically use it. The levels of the Table of Contents are identified by their left indent, so if you want the ebook to have a multi-level Table of Contents, make sure you create a properly indented Table of Contents in Word.\\n\\nIf no Table of Contents is found in the document, then a table of contents is automatically generated from the headings in the document. A heading is identified as something that has the Heading 1 or Heading 2, etc. style applied to it. These headings are turned into a Table of Contents with Heading 1 being the topmost level, Heading 2 the second level and so on.\\n\\n You can see the Table of Contents created by calibre by clicking the Table of Contents button in whatever viewer you are using to view the converted ebook. \\n\\n\\tDemonstration of DOCX support in calibre\\t1\\n\\n\\tText Formatting\\t2\\n\\n\\tInline formatting\\t2\\n\\n\\tFun with fonts\\t2\\n\\n\\tParagraph level formatting\\t2\\n\\n\\tTables\\t3\\n\\n\\tStructural Elements\\t5\\n\\n\\tFootnotes & Endnotes\\t5\\n\\n\\tDropcaps\\t5\\n\\n\\tLinks\\t5\\n\\n\\tTable of Contents\\t5\\n\\n\\tImages\\t7\\n\\n\\tLists\\t8\\n\\n\\tBulleted List\\t8\\n\\n\\tNumbered List\\t8\\n\\n\\tMulti-level Lists\\t8\\n\\n\\tContinued Lists\\t8\\n\\n\\n\\n\\n\\nImages\\n\\nImages can be of three main types. Inline images are images that are part of the normal text flow, like this image of a green dot . Inline images do not cause breaks in the text and are usually small in size. The next category of image is a floating image, one that “floats “ on the page and is surrounded by text. Word supports more types of floating images than are possible with current ebook technology, so the conversion maps floating images to simple left and right floats, as you can see with the left and right arrow images on the sides of this paragraph.\\n\\nThe final type of image is a “block” image, one that becomes a paragraph on its own and has no text on either side. Below is a centered green dot.\\n\\nCentered images like this are useful for large pictures that should be a focus of attention. \\n\\nGenerally, it is not possible to translate the exact positioning of images from a Word document to an ebook. That is because in Word, image positioning is specified in absolute units from the page boundaries.  There is no analogous technology in ebooks, so the conversion will usually end up placing the image either centered or floating close to the point in the text where it was inserted, not necessarily where it appears on the page in Word.\\n\\nLists\\n\\nAll types of lists are supported by the conversion, with the exception of lists that use fancy bullets, these get converted to regular bullets.\\n\\nBulleted List\\n\\nOne\\n\\nTwo\\n\\nNumbered List\\n\\nOne, with a very long line to demonstrate that the hanging indent for the list is working correctly\\n\\nTwo\\n\\nMulti-level Lists\\n\\nOne\\n\\nTwo\\n\\nThree\\n\\nFour with a very long line to demonstrate that the hanging indent for the list is working correctly.\\n\\nFive\\n\\nSix\\n\\nA Multi-level list with bullets:\\n\\nOne\\n\\nTwo\\n\\nThis bullet uses an image as the bullet item\\n\\nFour\\n\\nFive\\n\\nContinued Lists\\n\\nOne\\n\\nTwo\\n\\nAn interruption in our regularly scheduled listing, for this essential and very relevant public service announcement.\\n\\nWe now resume our normal programming\\n\\nFour')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf30f8f-fe1c-42df-b786-3414760689f6",
   "metadata": {},
   "source": [
    "### Load from Unstructured Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63893bdc-84ee-41ed-ac76-daf32b1fbdae",
   "metadata": {},
   "source": [
    "Sometimes, we need to load content from various text sources and formats without writing a separate loader for each one. Additionally, when a new file format emerges, we want to save time by not having to write a new loader for it. `UnstructuredFileLoader` addresses this need by supporting the loading of multiple file types. Currently, `UnstructuredFileLoader` can handle text files, PowerPoints, HTML, PDFs, images, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0fbf96-80bc-4991-9697-2d4f87d6f262",
   "metadata": {},
   "source": [
    "For example, we can load `.txt` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "864238a9-c534-4365-800f-5d1a61dc3dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content=\"1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual's contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\")]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredFileLoader(\"new-Policies.txt\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2868aed-a861-4943-aedb-badfd66774eb",
   "metadata": {},
   "source": [
    "We also can load `.md` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b39e0cf7-fc38-4dcf-9111-5978ba0c1382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'markdown-sample.md'}, page_content='An h1 header\\n\\nParagraphs are separated by a blank line.\\n\\n2nd paragraph. Italic, bold, and monospace. Itemized lists\\nlook like:\\n\\nthis one\\n\\nthat one\\n\\nthe other one\\n\\nNote that --- not considering the asterisk --- the actual text\\ncontent starts at 4-columns in.\\n\\nBlock quotes are\\nwritten like so.\\n\\nThey can span multiple paragraphs,\\nif you like.\\n\\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all\\nin chapters 12--14\"). Three dots ... will be converted to an ellipsis.\\nUnicode is supported. ☺\\n\\nAn h2 header\\n\\nHere\\'s a numbered list:\\n\\nfirst item\\n\\nsecond item\\n\\nthird item\\n\\nNote again how the actual text starts at 4 columns in (4 characters\\nfrom the left side). Here\\'s a code sample:\\n\\nAs you probably guessed, indented 4 spaces. By the way, instead of\\nindenting the block, you can use delimited blocks, if you like:\\n\\n~~~\\ndefine foobar() {\\n    print \"Welcome to flavor country!\";\\n}\\n~~~\\n\\n(which makes copying & pasting easier). You can optionally mark the\\ndelimited block for Pandoc to syntax highlight it:\\n\\n~~~python\\nimport time\\n\\nQuick, count to ten!\\n\\nfor i in range(10):\\n    # (but not too quick)\\n    time.sleep(0.5)\\n    print i\\n~~~\\n\\nAn h3 header\\n\\nNow a nested list:\\n\\nFirst, get these ingredients:\\n\\ncarrots\\ncelery\\nlentils\\n\\nBoil some water.\\n\\nDump everything in the pot and follow\\n    this algorithm:\\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)\\n\\nDo not bump wooden spoon or it will fall.\\n\\nNotice again how text always lines up on 4-space indents (including\\nthat last line which continues item 3 above).\\n\\nHere\\'s a link to a website, to a local\\ndoc, and to a section heading in the current\\ndoc. Here\\'s a footnote [^1].\\n\\n[^1]: Footnote text goes here.\\n\\nTables can look like this:\\n\\nsize  material      color\\n\\n9     leather       brown\\n10    hemp canvas   natural\\n11    glass         transparent\\n\\nTable: Shoes, their sizes, and what they\\'re made of\\n\\n(The above is the caption for the table.) Pandoc also supports\\nmulti-line tables:\\n\\nkeyword   text\\n\\nred       Sunsets, apples, and\\n          other red or reddish\\n          things.\\n\\ngreen     Leaves, grass, frogs\\n          and other things it\\'s\\n          not easy being.\\n\\nA horizontal rule follows.\\n\\nHere\\'s a definition list:\\n\\napples\\n  : Good for making applesauce.\\noranges\\n  : Citrus!\\ntomatoes\\n  : There\\'s no \"e\" in tomatoe.\\n\\nAgain, text is indented 4 spaces. (Put a blank line between each\\nterm/definition pair to spread things out more.)\\n\\nHere\\'s a \"line block\":\\n\\n| Line one\\n|   Line too\\n| Line tree\\n\\nand images can be specified like so:\\n\\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display\\nmath should get its own line and be put in in double-dollarsigns:\\n\\n$$I = \\\\int \\\\rho R^{2} dV$$\\n\\nAnd note that you can backslash-escape any punctuation characters\\nwhich you wish to be displayed literally, ex.: `foo`, *bar*, etc.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredFileLoader(\"markdown-sample.md\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca1ec8-4a54-40e2-9156-43f359be695a",
   "metadata": {},
   "source": [
    "#### Multiple files with different formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5d0af-9097-402b-ae3a-8c3f6545697a",
   "metadata": {},
   "source": [
    "We can even load a list of files with different formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "daaffd49-96f3-4075-9a7f-ce89d3e6b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"markdown-sample.md\", \"new-Policies.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d17bffc3-0b58-4380-bb78-d7692f430ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7873325c-0037-447b-a09b-e428a184ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35fefe8c-9ea6-485c-917c-8bfeae2f5016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': ['markdown-sample.md', 'new-Policies.txt']}, page_content='An h1 header\\n\\nParagraphs are separated by a blank line.\\n\\n2nd paragraph. Italic, bold, and monospace. Itemized lists\\nlook like:\\n\\nthis one\\n\\nthat one\\n\\nthe other one\\n\\nNote that --- not considering the asterisk --- the actual text\\ncontent starts at 4-columns in.\\n\\nBlock quotes are\\nwritten like so.\\n\\nThey can span multiple paragraphs,\\nif you like.\\n\\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all\\nin chapters 12--14\"). Three dots ... will be converted to an ellipsis.\\nUnicode is supported. ☺\\n\\nAn h2 header\\n\\nHere\\'s a numbered list:\\n\\nfirst item\\n\\nsecond item\\n\\nthird item\\n\\nNote again how the actual text starts at 4 columns in (4 characters\\nfrom the left side). Here\\'s a code sample:\\n\\nAs you probably guessed, indented 4 spaces. By the way, instead of\\nindenting the block, you can use delimited blocks, if you like:\\n\\n~~~\\ndefine foobar() {\\n    print \"Welcome to flavor country!\";\\n}\\n~~~\\n\\n(which makes copying & pasting easier). You can optionally mark the\\ndelimited block for Pandoc to syntax highlight it:\\n\\n~~~python\\nimport time\\n\\nQuick, count to ten!\\n\\nfor i in range(10):\\n    # (but not too quick)\\n    time.sleep(0.5)\\n    print i\\n~~~\\n\\nAn h3 header\\n\\nNow a nested list:\\n\\nFirst, get these ingredients:\\n\\ncarrots\\ncelery\\nlentils\\n\\nBoil some water.\\n\\nDump everything in the pot and follow\\n    this algorithm:\\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)\\n\\nDo not bump wooden spoon or it will fall.\\n\\nNotice again how text always lines up on 4-space indents (including\\nthat last line which continues item 3 above).\\n\\nHere\\'s a link to a website, to a local\\ndoc, and to a section heading in the current\\ndoc. Here\\'s a footnote [^1].\\n\\n[^1]: Footnote text goes here.\\n\\nTables can look like this:\\n\\nsize  material      color\\n\\n9     leather       brown\\n10    hemp canvas   natural\\n11    glass         transparent\\n\\nTable: Shoes, their sizes, and what they\\'re made of\\n\\n(The above is the caption for the table.) Pandoc also supports\\nmulti-line tables:\\n\\nkeyword   text\\n\\nred       Sunsets, apples, and\\n          other red or reddish\\n          things.\\n\\ngreen     Leaves, grass, frogs\\n          and other things it\\'s\\n          not easy being.\\n\\nA horizontal rule follows.\\n\\nHere\\'s a definition list:\\n\\napples\\n  : Good for making applesauce.\\noranges\\n  : Citrus!\\ntomatoes\\n  : There\\'s no \"e\" in tomatoe.\\n\\nAgain, text is indented 4 spaces. (Put a blank line between each\\nterm/definition pair to spread things out more.)\\n\\nHere\\'s a \"line block\":\\n\\n| Line one\\n|   Line too\\n| Line tree\\n\\nand images can be specified like so:\\n\\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display\\nmath should get its own line and be put in in double-dollarsigns:\\n\\n$$I = \\\\int \\\\rho R^{2} dV$$\\n\\nAnd note that you can backslash-escape any punctuation characters\\nwhich you wish to be displayed literally, ex.: `foo`, *bar*, etc.\\n\\n1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual\\'s contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates\\' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60bdc92-b936-4b3e-93a4-6e570e0b1159",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f7a9c-988c-4fea-a47f-33dc8df7814c",
   "metadata": {},
   "source": [
    "### Exercise 1 - Try to use other PDF loaders\n",
    "\n",
    "There are many other PDF loaders in LangChain, for example, `PyPDFium2Loader`. Can you use this PDF loader to load the PDF and see the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92b7c0f9-077b-4b47-98f2-f60c35e036e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdfium2\n",
      "  Downloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
      "Downloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2\n",
      "Successfully installed pypdfium2-5.1.0\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "!pip install pypdfium2\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "loader = PyPDFium2Loader(pdf_url)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e7360-16ca-4215-906a-be92550613d6",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "!pip install pypdfium2\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "loader = PyPDFium2Loader(pdf_url)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37792c8e-6a2b-4c47-b98f-3b339e1bd1bc",
   "metadata": {},
   "source": [
    "### Exercise 2 - Load from Arxiv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff9996-e0a9-4640-875e-44638ffd2a29",
   "metadata": {},
   "source": [
    "Sometimes we have paper that we want to load from Arxiv, can you load this [paper](https://arxiv.org/abs/1605.08386) using `ArxivLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e16a5f91-4e5c-4375-91a7-943a035f3a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in /home/abolfazl/.local/lib/python3.10/site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/abolfazl/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abolfazl/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abolfazl/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abolfazl/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2025.11.12)\n",
      "Downloading arxiv-2.3.1-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "\u001b[33m  DEPRECATION: Building 'sgmllib3k' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sgmllib3k'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6090 sha256=aa0fa8ca668a5498576e178a91e1876d513dee0003d3bfe995b52a1fd4ce11f0\n",
      "  Stored in directory: /home/abolfazl/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [arxiv]32m1/3\u001b[0m [feedparser]\n",
      "\u001b[1A\u001b[2KSuccessfully installed arxiv-2.3.1 feedparser-6.0.12 sgmllib3k-1.0.0\n",
      "arXiv:1605.08386v1  [math.CO]  26 May 2016\n",
      "HEAT-BATH RANDOM WALKS WITH MARKOV BASES\n",
      "CAPRICE STANLEY AND TOBIAS WINDISCH\n",
      "Abstract. Graphs on lattice points are studied whose edges come from a ﬁnite set of\n",
      "allowed moves of arbitrary length. We show that the diameter of these graphs on ﬁbers of a\n",
      "ﬁxed integer matrix can be bounded from above by a constant. We then study the mixing\n",
      "behaviour of heat-bath random walks on these graphs. We also state explicit conditions\n",
      "on the set of moves so that the heat-bath random walk, a generalization of the Glauber\n",
      "dynamics, is an expander in ﬁxed dimension.\n",
      "Contents\n",
      "1.\n",
      "Introduction\n",
      "1\n",
      "2.\n",
      "Graphs and statistics\n",
      "3\n",
      "3.\n",
      "Bounds on the diameter\n",
      "4\n",
      "4.\n",
      "Heat-bath random walks\n",
      "8\n",
      "5.\n",
      "Augmenting Markov bases\n",
      "14\n",
      "References\n",
      "19\n",
      "1. Introduction\n",
      "A ﬁber graph is a graph on the ﬁnitely many lattice points F ⊂Zd of a polytope where\n",
      "two lattice points are connected by an edge if their diﬀerence lies in a ﬁnite set of allowed\n",
      "moves M ⊂Zd. The implicit structure of these graphs \n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "!pip install arxiv\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"1605.08386\", load_max_docs=2).load()\n",
    "\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a6ddd1-0fb4-4822-9996-a79497c9f7f1",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "!pip install arxiv\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"1605.08386\", load_max_docs=2).load()\n",
    "\n",
    "print(docs[0].page_content[:1000])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829e5eb-778f-4130-b836-2c4cdb7abaf6",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451b656-ab11-4f23-9f14-004e68dba605",
   "metadata": {},
   "source": [
    "[Kang Wang](https://www.linkedin.com/in/kangwang95/)\n",
    "\n",
    "Kang Wang is a Data Scientist in IBM. He is also a PhD Candidate in the University of Waterloo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1f382-2ccb-4e9f-b847-57ab4def880e",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74614962-4e52-4c1e-b0cc-336c530de133",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/)\n",
    "\n",
    "Joseph has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Hailey Quach](https://author.skills.network/instructors/hailey_quach)\n",
    "\n",
    "Hailey is a Data Scientist at IBM. She is also an undergraduate student at Concordia University, Montreal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf90c676-6b73-40c0-8350-b326cb4e3c2d",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 01\n",
    "# ## Document Loader\n",
    "# def document_loader(file):\n",
    "#     loader = PyPDFLoader(file.name)\n",
    "#     loaded_document = loader.load()\n",
    "#     return loaded_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 04\n",
    "# # Vector db\n",
    "# def vector_database(chunks):\n",
    "#     embedding_model = watsonx_embedding()\n",
    "#     vectordb = Chroma.from_documents(chunks, embedding_model)\n",
    "#     return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 05\n",
    "# ## Retriever\n",
    "# def retriever(file):\n",
    "#     splits = document_loader(file)\n",
    "#     chunks = text_splitter(splits)\n",
    "#     vectordb = vector_database(chunks)\n",
    "#     retriever = vectordb.as_retriever()\n",
    "#     return retriever\n",
    "\n",
    "\n",
    "# ## QA Chain\n",
    "# def retriever_qa(file, query)\n",
    "#     llm = get_llm()\n",
    "#     retriever_obj = retriever(file)\n",
    "#     qa = RetrievalQA.from_chain_type(\n",
    "#         llm=llm,\n",
    "#         chain_type=\"stuff\",\n",
    "#         retriever=retriever_obj,\n",
    "#         return_source_documents=False\n",
    "#     )\n",
    "\n",
    "#     response = qa.invoke(query)\n",
    "#     return response['result']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "prev_pub_hash": "8ccd87848ab9e79d16c766e68c2292b6bf1eff17098bb52f22c15a7b9da59990"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
